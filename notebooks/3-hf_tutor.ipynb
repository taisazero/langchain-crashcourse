{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lancedb\n",
    "from langchain.vectorstores import LanceDB\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import LanceDB\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gdown\n",
      "  Downloading gdown-4.7.1-py3-none-any.whl (15 kB)\n",
      "Collecting filelock (from gdown)\n",
      "  Obtaining dependency information for filelock from https://files.pythonhosted.org/packages/5e/5d/97afbafd9d584ff1b45fcb354a479a3609bd97f912f8f1f6c563cb1fae21/filelock-3.12.4-py3-none-any.whl.metadata\n",
      "  Downloading filelock-3.12.4-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\erfan\\miniconda3\\envs\\langcourse\\lib\\site-packages (from gdown) (2.31.0)\n",
      "Requirement already satisfied: six in c:\\users\\erfan\\miniconda3\\envs\\langcourse\\lib\\site-packages (from gdown) (1.16.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\erfan\\miniconda3\\envs\\langcourse\\lib\\site-packages (from gdown) (4.66.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\erfan\\miniconda3\\envs\\langcourse\\lib\\site-packages (from gdown) (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\erfan\\miniconda3\\envs\\langcourse\\lib\\site-packages (from beautifulsoup4->gdown) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\erfan\\miniconda3\\envs\\langcourse\\lib\\site-packages (from requests[socks]->gdown) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\erfan\\miniconda3\\envs\\langcourse\\lib\\site-packages (from requests[socks]->gdown) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\erfan\\miniconda3\\envs\\langcourse\\lib\\site-packages (from requests[socks]->gdown) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\erfan\\miniconda3\\envs\\langcourse\\lib\\site-packages (from requests[socks]->gdown) (2023.7.22)\n",
      "Collecting PySocks!=1.5.7,>=1.5.6 (from requests[socks]->gdown)\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\erfan\\miniconda3\\envs\\langcourse\\lib\\site-packages (from tqdm->gdown) (0.4.6)\n",
      "Downloading filelock-3.12.4-py3-none-any.whl (11 kB)\n",
      "Installing collected packages: PySocks, filelock, gdown\n",
      "Successfully installed PySocks-1.7.1 filelock-3.12.4 gdown-4.7.1\n"
     ]
    }
   ],
   "source": [
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "gdown.download_folder(\n",
    "    \"https://drive.google.com/drive/folders/1SkI0ttpMNTVHp6ear6cTLDooRmtqvmVo?usp=sharing\",\n",
    "      quiet=True,\n",
    "      output=\"../\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = lancedb.connect('../.lancedb')\n",
    "table = db.open_table('hf_docs')\n",
    "embedding_fn = OpenAIEmbeddings(chunk_size=200)\n",
    "vectorstore = LanceDB(table, embedding_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "qa = RetrievalQA.from_chain_type(llm=ChatOpenAI(temperature=0, model='gpt-3.5-turbo-16k'), chain_type=\"stuff\", retriever=vectorstore.as_retriever(search_kwargs=dict(k=5)), verbose=True, return_source_documents = True)\n",
    "# qa = RetrievalQA.from_chain_type(llm=OpenAI(temperature=0), chain_type=\"stuff\", retriever=vectorstore.as_retriever(search_kwargs=dict(k=2)), verbose=True, return_source_documents = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the context length of Llama2?\"\n",
    "answer = qa({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What is the context length of Llama2?',\n",
       " 'result': \"The context length of Llama2 is not explicitly mentioned in the given context. Therefore, I don't have information about the specific context length of Llama2.\",\n",
       " 'source_documents': [Document(page_content='Utilities for Trainer Utilities for Generation Utilities for Image Processors\\nUtilities for Audio processing General Utilities Utilities for Time Series\\nJoin the Hugging Face community and get access to the augmented documentation\\nexperience Collaborate on models, datasets and Spaces Faster examples with\\naccelerated inference Switch between documentation themes Sign Up to get\\nstarted Llama2 Overview The Llama2 model was proposed in LLaMA: Open\\nFoundation and Fine-Tuned Chat Models by Hugo Touvron, Louis Martin, Kevin\\nStone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher,\\nCristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude\\nFernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami,\\nNaman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin\\nKardas, Viktor Kerkez Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit\\nSingh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich,\\nYinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushka rMishra, Igor\\nMolybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan\\nSaladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian,\\nXiaoqing EllenTan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan,\\nPuxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur,\\nSharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, Thomas\\nScialom. It is a collection of foundation language models ranging from 7B to\\n70B parameters, with checkpoints finetuned for chat application! The abstract\\nfrom the paper is the following: In this work, we develop and release Llama 2,\\na collection of pretrained and fine-tuned large language models (LLMs) ranging\\nin scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called\\nLlama 2-Chat, are optimized for dialogue use cases. Our models outperform\\nopen-source chat models on most benchmarks we tested, and based on our human\\nevaluations for helpfulness and safety, may be a suitable substitute for\\nclosed-source models. We provide a detailed description of our approach to\\nfine-tuning and safety improvements of Llama 2-Chat in order to enable the\\ncommunity to build on our work and contribute to the responsible development\\nof LLMs. Checkout all Llama2 models here The Llama2 models were trained using\\nbfloat16, but the original inference uses float16. The checkpoints uploaded on\\nthe hub use torch_dtype = ‚Äòfloat16‚Äôwhich will be used by theAutoModelAPI to\\ncast the checkpoints fromtorch.float32totorch.float16`. The dtype of the\\nonline weights is mostly irrelevant, unless you are using torch_dtype=\"auto\"\\nwhen initializing a model using model =\\nAutoModelForCausalLM.from_pretrained(\"path\", torch_dtype = \"auto\"). The reason\\nis that the model will first be downloaded ( using the dtype of the\\ncheckpoints online) then it will be casted to the default dtype of torch\\n(becomes torch.float32) and finally, if there is a torch_dtype provided in the\\nconfig, it will be used. Training the model in float16 is not recommended and\\nknown to produce nan, as such the model should be trained in bfloat16. Tips:\\nWeights for the Llama2 models can be obtained by filling out this form The\\narchitecture is very similar to the first Llama, with the addition of Grouped\\nQuery Attention (GQA) following this paper Setting config.pretraining_tp to a\\nvalue different than 1 will activate the more accurate but slower computation\\nof the linear layers, which should better match the original logits. The\\noriginal model uses pad_id = -1 which means that there is no padding token. We\\ncan‚Äôt have the same logic, make sure to add a padding token using\\ntokenizer.add_special_tokens({\"pad_token\":\"\"}) and resize the token embedding\\naccordingly. You should also set the model.config.pad_token_id. The\\nembed_tokens layer of the model is initialized with self.embed_tokens =', metadata={'vector': array([-0.02836467,  0.02697599,  0.00048382, ..., -0.00127512,\n",
       "         -0.00710225, -0.02353381], dtype=float32), 'id': '0c07a68f-8702-4218-8377-c5e5656fd0ae', '_distance': 0.4277893304824829}),\n",
       "  Document(page_content='Llama2 Hugging Face Models Datasets Spaces Docs Solutions Pricing Log In Sign\\nUp Transformers documentation Llama2 Transformers Search documentation\\nmainv4.33.2v4.32.1v4.31.0v4.30.0v4.29.1v4.28.1v4.27.2v4.26.1v4.25.1v4.24.0v4.23.1v4.22.2v4.21.3v4.20.1v4.19.4v4.18.0v4.17.0v4.16.2v4.15.0v4.14.1v4.13.0v4.12.5v4.11.3v4.10.1v4.9.2v4.8.2v4.7.0v4.6.0v4.5.1v4.4.2v4.3.3v4.2.2v4.1.1v4.0.1v3.5.1v3.4.0v3.3.1v3.2.0v3.1.0v3.0.2v2.11.0v2.10.0v2.9.1v2.8.0v2.7.0v2.6.0v2.5.1v2.4.1v2.3.0v2.2.2v2.1.1v2.0.0v1.2.0v1.1.0v1.0.0doc-\\nbuilder-html DEENESFRITKOPTZH Get started ü§ó Transformers Quick tour\\nInstallation Tutorials Run inference with pipelines Write portable code with\\nAutoClass Preprocess data Fine-tune a pretrained model Train with a script Set\\nup distributed training with ü§ó Accelerate Load and train adapters with ü§ó PEFT\\nShare your model Agents Generation with LLMs Task Guides Natural Language\\nProcessing Audio Computer Vision Multimodal Generation Developer guides Use\\nfast tokenizers from ü§ó Tokenizers Run inference with multilingual models Use\\nmodel-specific APIs Share a custom model Run training on Amazon SageMaker\\nExport to ONNX Export to TFLite Export to TorchScript Benchmarks Notebooks\\nwith examples Community resources Custom Tools and Prompts Troubleshoot\\nPerformance and scalability Overview Efficient training techniques Methods and\\ntools for efficient training on a single GPU Multiple GPUs and parallelism\\nEfficient training on CPU Distributed CPU training Training on TPUs Training\\non TPU with TensorFlow Training on Specialized Hardware Custom hardware for\\ntraining Hyperparameter Search using Trainer API Optimizing inference\\nInference on CPU Inference on one GPU Inference on many GPUs Inference on\\nSpecialized Hardware Instantiating a big model Troubleshooting XLA Integration\\nfor TensorFlow Models Optimize inference using `torch.compile()` Contribute\\nHow to contribute to transformers? How to add a model to ü§ó Transformers? How\\nto convert a ü§ó Transformers model to TensorFlow? How to add a pipeline to ü§ó\\nTransformers? Testing Checks on a Pull Request Conceptual guides Philosophy\\nGlossary What ü§ó Transformers can do How ü§ó Transformers solve tasks The\\nTransformer model family Summary of the tokenizers Attention mechanisms\\nPadding and truncation BERTology Perplexity of fixed-length models Pipelines\\nfor webserver inference Model training anatomy API Main Classes Agents and\\nTools Auto Classes Callbacks Configuration Data Collator Keras callbacks\\nLogging Models Text Generation ONNX Optimization Model outputs Pipelines\\nProcessors Quantization Tokenizer Trainer DeepSpeed Integration Feature\\nExtractor Image Processor Models Text models ALBERT BART BARThez BARTpho BERT\\nBertGeneration BertJapanese Bertweet BigBird BigBirdPegasus BioGpt Blenderbot\\nBlenderbot Small BLOOM BORT ByT5 CamemBERT CANINE CodeGen CodeLlama ConvBERT\\nCPM CPMANT CTRL DeBERTa DeBERTa-v2 DialoGPT DistilBERT DPR ELECTRA Encoder\\nDecoder Models ERNIE ErnieM ESM Falcon FLAN-T5 FLAN-UL2 FlauBERT FNet FSMT\\nFunnel Transformer GPT GPT Neo GPT NeoX GPT NeoX Japanese GPT-J GPT2\\nGPTBigCode GPTSAN Japanese GPTSw3 HerBERT I-BERT Jukebox LED LLaMA Llama2\\nLongformer LongT5 LUKE M2M100 MarianMT MarkupLM MBart and MBart-50 MEGA\\nMegatronBERT MegatronGPT2 mLUKE MobileBERT MPNet MPT MRA MT5 MVP NEZHA NLLB\\nNLLB-MoE Nystr√∂mformer Open-Llama OPT Pegasus PEGASUS-X PhoBERT PLBart\\nProphetNet QDQBert RAG REALM Reformer RemBERT RetriBERT RoBERTa RoBERTa-\\nPreLayerNorm RoCBert RoFormer RWKV Splinter SqueezeBERT SwitchTransformers T5\\nT5v1.1 TAPEX Transformer XL UL2 UMT5 X-MOD XGLM XLM XLM-ProphetNet XLM-RoBERTa\\nXLM-RoBERTa-XL XLM-V XLNet YOSO Vision models Audio models Multimodal models\\nReinforcement learning models Time series models Graph models Internal Helpers\\nCustom Layers and Utilities Utilities for pipelines Utilities for Tokenizers\\nUtilities for Trainer Utilities for Generation Utilities for Image Processors\\nUtilities for Audio processing General Utilities Utilities for Time Series', metadata={'vector': array([-0.02751405,  0.00533659, -0.01040441, ...,  0.00659913,\n",
       "         -0.0246424 , -0.0194508 ], dtype=float32), 'id': '72a58fc2-5a81-4742-a8c2-958828bfa460', '_distance': 0.428567498922348}),\n",
       "  Document(page_content=\"4, 5, 6,' Further resources While the autoregressive generation process is\\nrelatively straightforward, making the most out of your LLM can be a\\nchallenging endeavor because there are many moving parts. For your next steps\\nto help you dive deeper into LLM usage and understanding: Advanced generate\\nusage Guide on how to control different generation methods, how to set up the\\ngeneration configuration file, and how to stream the output; API reference on\\nGenerationConfig, generate(), and generate-related classes. LLM leaderboards\\nOpen LLM Leaderboard, which focuses on the quality of the open-source models;\\nOpen LLM-Perf Leaderboard, which focuses on LLM throughput. Latency and\\nthroughput Guide on dynamic quantization, which shows you how to drastically\\nreduce your memory requirements. Related libraries text-generation-inference,\\na production-ready server for LLMs; optimum, an extension of ü§ó Transformers\\nthat optimizes for specific hardware devices. ‚ÜêAgents Text classification‚Üí\\nGeneration with LLMs Generate text Common pitfalls Generated output is too\\nshort/long Incorrect generation mode Wrong padding side Further resources\\nAdvanced generate usage LLM leaderboards Latency and throughput Related\\nlibraries\", metadata={'vector': array([-0.00903042,  0.0234562 , -0.02593205, ...,  0.01245082,\n",
       "          0.00172451, -0.02158142], dtype=float32), 'id': 'b8e72ff7-3e5f-4130-b2f4-9e372a41333d', '_distance': 0.4577292203903198}),\n",
       "  Document(page_content='X=(x0,x1,‚Ä¶,xt)X = (x_0, x_1, \\\\dots, x_t)X=(x0\\u200b,x1\\u200b,‚Ä¶,xt\\u200b), then the perplexity\\nof XXX is, PPL(X)=exp\\u2061{‚àí1t‚àëitlog\\u2061pŒ∏(xi‚à£x When working with approximate models,\\nhowever, we typically have a constraint on the number of tokens the model can\\nprocess. The largest version of GPT-2, for example, has a fixed length of 1024\\ntokens, so we cannot calculate pŒ∏(xt‚à£x This is quick to compute since the\\nperplexity of each segment can be computed in one forward pass, but serves as\\na poor approximation of the fully-factorized perplexity and will typically\\nyield a higher (worse) PPL because the model will have less context at most of\\nthe prediction steps. Instead, the PPL of fixed-length models should be\\nevaluated with a sliding-window strategy. This involves repeatedly sliding the\\ncontext window so that the model has more context when making each prediction.\\nThis is a closer approximation to the true decomposition of the sequence\\nprobability and will typically yield a more favorable score. The downside is\\nthat it requires a separate forward pass for each token in the corpus. A good\\npractical compromise is to employ a strided sliding window, moving the context\\nby larger strides rather than sliding by 1 token a time. This allows\\ncomputation to proceed much faster while still giving the model a large\\ncontext to make predictions at each step. Example: Calculating perplexity with\\nGPT-2 in ü§ó Transformers Let‚Äôs demonstrate this process with GPT-2. Copied from\\ntransformers import GPT2LMHeadModel, GPT2TokenizerFast device = \"cuda\"\\nmodel_id = \"gpt2-large\" model =\\nGPT2LMHeadModel.from_pretrained(model_id).to(device) tokenizer =\\nGPT2TokenizerFast.from_pretrained(model_id) We‚Äôll load in the WikiText-2\\ndataset and evaluate the perplexity using a few different sliding-window\\nstrategies. Since this dataset is small and we‚Äôre just doing one forward pass\\nover the set, we can just load and encode the entire dataset in memory. Copied\\nfrom datasets import load_dataset test = load_dataset(\"wikitext\",\\n\"wikitext-2-raw-v1\", split=\"test\") encodings =\\ntokenizer(\"\\\\n\\\\n\".join(test[\"text\"]), return_tensors=\"pt\") With ü§ó Transformers,\\nwe can simply pass the input_ids as the labels to our model, and the average\\nnegative log-likelihood for each token is returned as the loss. With our\\nsliding window approach, however, there is overlap in the tokens we pass to\\nthe model at each iteration. We don‚Äôt want the log-likelihood for the tokens\\nwe‚Äôre just treating as context to be included in our loss, so we can set these\\ntargets to -100 so that they are ignored. The following is an example of how\\nwe could do this with a stride of 512. This means that the model will have at\\nleast 512 tokens for context when calculating the conditional likelihood of\\nany one token (provided there are 512 preceding tokens available to condition\\non). Copied import torch from tqdm import tqdm max_length =\\nmodel.config.n_positions stride = 512 seq_len = encodings.input_ids.size(1)\\nnlls = [] prev_end_loc = 0 for begin_loc in tqdm(range(0, seq_len, stride)):\\nend_loc = min(begin_loc + max_length, seq_len) trg_len = end_loc -\\nprev_end_loc # may be different from stride on last loop input_ids =\\nencodings.input_ids[:, begin_loc:end_loc].to(device) target_ids =\\ninput_ids.clone() target_ids[:, :-trg_len] = -100 with torch.no_grad():\\noutputs = model(input_ids, labels=target_ids) # loss is calculated using\\nCrossEntropyLoss which averages over valid labels # N.B. the model only\\ncalculates loss over trg_len - 1 labels, because it internally shifts the\\nlabels # to the left by 1. neg_log_likelihood = outputs.loss\\nnlls.append(neg_log_likelihood) prev_end_loc = end_loc if end_loc == seq_len:\\nbreak ppl = torch.exp(torch.stack(nlls).mean()) Running this with the stride\\nlength equal to the max input length is equivalent to the suboptimal, non-\\nsliding-window strategy we discussed above. The smaller the stride, the more\\ncontext the model will have in making each prediction, and the better the', metadata={'vector': array([ 0.00912523,  0.00627665,  0.0163514 , ...,  0.01549962,\n",
       "          0.00576348, -0.03778555], dtype=float32), 'id': '546e5ba8-96ce-4b8e-8d5a-1563862a35e2', '_distance': 0.4808694124221802}),\n",
       "  Document(page_content='Open-Llama Hugging Face Models Datasets Spaces Docs Solutions Pricing Log In\\nSign Up Transformers documentation Open-Llama Transformers Search\\ndocumentation\\nmainv4.33.2v4.32.1v4.31.0v4.30.0v4.29.1v4.28.1v4.27.2v4.26.1v4.25.1v4.24.0v4.23.1v4.22.2v4.21.3v4.20.1v4.19.4v4.18.0v4.17.0v4.16.2v4.15.0v4.14.1v4.13.0v4.12.5v4.11.3v4.10.1v4.9.2v4.8.2v4.7.0v4.6.0v4.5.1v4.4.2v4.3.3v4.2.2v4.1.1v4.0.1v3.5.1v3.4.0v3.3.1v3.2.0v3.1.0v3.0.2v2.11.0v2.10.0v2.9.1v2.8.0v2.7.0v2.6.0v2.5.1v2.4.1v2.3.0v2.2.2v2.1.1v2.0.0v1.2.0v1.1.0v1.0.0doc-\\nbuilder-html DEENESFRITKOPTZH Get started ü§ó Transformers Quick tour\\nInstallation Tutorials Run inference with pipelines Write portable code with\\nAutoClass Preprocess data Fine-tune a pretrained model Train with a script Set\\nup distributed training with ü§ó Accelerate Load and train adapters with ü§ó PEFT\\nShare your model Agents Generation with LLMs Task Guides Natural Language\\nProcessing Audio Computer Vision Multimodal Generation Developer guides Use\\nfast tokenizers from ü§ó Tokenizers Run inference with multilingual models Use\\nmodel-specific APIs Share a custom model Run training on Amazon SageMaker\\nExport to ONNX Export to TFLite Export to TorchScript Benchmarks Notebooks\\nwith examples Community resources Custom Tools and Prompts Troubleshoot\\nPerformance and scalability Overview Efficient training techniques Methods and\\ntools for efficient training on a single GPU Multiple GPUs and parallelism\\nEfficient training on CPU Distributed CPU training Training on TPUs Training\\non TPU with TensorFlow Training on Specialized Hardware Custom hardware for\\ntraining Hyperparameter Search using Trainer API Optimizing inference\\nInference on CPU Inference on one GPU Inference on many GPUs Inference on\\nSpecialized Hardware Instantiating a big model Troubleshooting XLA Integration\\nfor TensorFlow Models Optimize inference using `torch.compile()` Contribute\\nHow to contribute to transformers? How to add a model to ü§ó Transformers? How\\nto convert a ü§ó Transformers model to TensorFlow? How to add a pipeline to ü§ó\\nTransformers? Testing Checks on a Pull Request Conceptual guides Philosophy\\nGlossary What ü§ó Transformers can do How ü§ó Transformers solve tasks The\\nTransformer model family Summary of the tokenizers Attention mechanisms\\nPadding and truncation BERTology Perplexity of fixed-length models Pipelines\\nfor webserver inference Model training anatomy API Main Classes Agents and\\nTools Auto Classes Callbacks Configuration Data Collator Keras callbacks\\nLogging Models Text Generation ONNX Optimization Model outputs Pipelines\\nProcessors Quantization Tokenizer Trainer DeepSpeed Integration Feature\\nExtractor Image Processor Models Text models ALBERT BART BARThez BARTpho BERT\\nBertGeneration BertJapanese Bertweet BigBird BigBirdPegasus BioGpt Blenderbot\\nBlenderbot Small BLOOM BORT ByT5 CamemBERT CANINE CodeGen CodeLlama ConvBERT\\nCPM CPMANT CTRL DeBERTa DeBERTa-v2 DialoGPT DistilBERT DPR ELECTRA Encoder\\nDecoder Models ERNIE ErnieM ESM Falcon FLAN-T5 FLAN-UL2 FlauBERT FNet FSMT\\nFunnel Transformer GPT GPT Neo GPT NeoX GPT NeoX Japanese GPT-J GPT2\\nGPTBigCode GPTSAN Japanese GPTSw3 HerBERT I-BERT Jukebox LED LLaMA Llama2\\nLongformer LongT5 LUKE M2M100 MarianMT MarkupLM MBart and MBart-50 MEGA\\nMegatronBERT MegatronGPT2 mLUKE MobileBERT MPNet MPT MRA MT5 MVP NEZHA NLLB\\nNLLB-MoE Nystr√∂mformer Open-Llama OPT Pegasus PEGASUS-X PhoBERT PLBart\\nProphetNet QDQBert RAG REALM Reformer RemBERT RetriBERT RoBERTa RoBERTa-\\nPreLayerNorm RoCBert RoFormer RWKV Splinter SqueezeBERT SwitchTransformers T5\\nT5v1.1 TAPEX Transformer XL UL2 UMT5 X-MOD XGLM XLM XLM-ProphetNet XLM-RoBERTa\\nXLM-RoBERTa-XL XLM-V XLNet YOSO Vision models Audio models Multimodal models\\nReinforcement learning models Time series models Graph models Internal Helpers\\nCustom Layers and Utilities Utilities for pipelines Utilities for Tokenizers\\nUtilities for Trainer Utilities for Generation Utilities for Image Processors\\nUtilities for Audio processing General Utilities Utilities for Time Series', metadata={'vector': array([-0.0196955 , -0.00452034, -0.00573095, ...,  0.00507609,\n",
       "         -0.01541941, -0.01939815], dtype=float32), 'id': '82da878d-6e02-4e17-93da-b5d225f93284', '_distance': 0.48426705598831177})]}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write answer to file\n",
    "with open(\"answer.txt\", \"w\") as f:\n",
    "    f.write(answer['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "* https://lancedb.github.io/lancedb/notebooks/code_qa_bot/\n",
    "* https://github.com/langchain-ai/chat-langchain/tree/master"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langcourse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
