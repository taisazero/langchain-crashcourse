{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/taisazero/langchain-crashcourse/blob/main/notebooks/2-simple_agent_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Activity 1: Building a Simple Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction\n",
    "In this lab, we will be building a simple langchain agent that can use a set of pre-defined tools. We will experiment with adding and removing tools and observe model robustness. At the end of this notebook, we will put our agent together in a chainlit web interface.\n",
    "\n",
    "**Note:** Refer to the [documentation](https://python.langchain.com/docs/get_started/introduction) and the [deeplearning.ai course](https://learn.deeplearning.ai/langchain) as needed. You are also allowed to ask the TA's and the instructor for help during the class. Please do so if you get stuck and aim to finish the lab within the class time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to install our dependencies for this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: install langchain\n",
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing other dependencies\n",
    "!pip install chainlit\n",
    "!pip install python-dotenv\n",
    "!pip install tiktoken\n",
    "!pip install google-search-results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: create a .env file with the following variables and then load them using dotenv.\n",
    "# * SERPAPI_API_KEY (Make one by signing up at https://serpapi.com/)\n",
    "# * OPENAI_API_KEY\n",
    "# Note: If you are using the Llama2 hosted model, set the OPENAI_API_KEY to the key provided \n",
    "# and also set OPENAI_API_BASE to the base url provided.\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Create a ChatOpenAI LLM with greedy decoding, a model name of gpt-3.5-turbo, and a max_tokens of 500.\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "llm = ChatOpenAI(\n",
    "        temperature= 0,\n",
    "        model_name='gpt-3.5-turbo',\n",
    "        max_tokens= 500,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: create a memory that uses the llm and has a max_token_limit of 250.\n",
    "# Tip: Set `return_messages` to `True` in the memory so that the agent returns the messages it generates\n",
    "# Tip: The user message in the prompt is stored in the `input` key and the chat history is stored in the `chat_history` key.\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "conversational_memory = ConversationBufferMemory(\n",
    "        llm=llm,\n",
    "        input_key='input',\n",
    "        memory_key='chat_history',\n",
    "        max_token_limit=250,\n",
    "        return_messages=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: load the following tools ['llm-math', 'terminal', 'python_repl', 'serpapi']\n",
    "from langchain.agents import load_tools\n",
    "tools = load_tools(['llm-math', 'terminal', 'python_repl', 'serpapi'], llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.initialize import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    " #TODO: initialize agent\n",
    "    # Hint: There is a parameter that allows you to set the maximum number of iterations, \n",
    "        # this is useful to limit the number of messages the agent generates and hence $ spent on OAI credits.\n",
    "    # Hint: CHAT_CONVERSATIONAL_REACT_DESCRIPTION needs the memory to return its messages.\n",
    "agent = initialize_agent(\n",
    "        llm=llm,\n",
    "        memory=conversational_memory,\n",
    "        tools=tools,\n",
    "        verbose=True,\n",
    "        agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION, \n",
    "        max_iterations=3,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: test the agent\n",
    "agent.run(\"Hello there! How are you doing?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: run the agent with a set of 5 prompts of your choice and note your observations.\n",
    "prompts = [\n",
    "    'Who is the current president of the United States?',\n",
    "    'Do you have any recommendations for a good restaurant in Charlotte, NC?',\n",
    "    'How many apples are there if I have 5 apples and I eat 2 apples?'\n",
    "    'Compute the average temperature for Charlotte, NC for the past week. Be sure to use the appropriate tools.'\n",
    "    'What can you do with tools?'\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f'Prompt: {prompt}\\t\\n {agent.run(prompt)}\\n\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations and Insights\n",
    "**Question**:\n",
    "* Try using each tool, mention what works and what doesn't. Why do you think that is? What are some of the limitations of this agent? What are some of the limitations of the tools?\n",
    "* For searching, try something like `who is the current president of the US` or `Are there any middle eastern arabic restaurants around Charlotte?`. For `math` give it a math word problem e.g. `How many apples are there if I have 5 apples and I buy 10 more?`. For `python-repl` try asking it to make a table of temperatures and compute the average.\n",
    "* Do you notice any errors?\n",
    "\n",
    "**Answer**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Re-initialize the agent with the type `CHAT_CONVERSATIONAL_REACT_DESCRIPTION` \n",
    "# and set `handle_parsing_errors` to `True`\n",
    "agent = initialize_agent(\n",
    "        llm=llm,\n",
    "        memory=conversational_memory,\n",
    "        tools=tools,\n",
    "        verbose=True,\n",
    "        agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION, \n",
    "        handle_parsing_errors=True,\n",
    "        max_iterations=3,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: test the agent\n",
    "agent.run(\"Hello there! How are you doing?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: run the agent with a set of 5 prompts of your choice and note your observations.\n",
    "prompts = [\n",
    "    'Who is the current president of the United States?',\n",
    "    'Do you have any recommendations for a good restaurant in Charlotte, NC?',\n",
    "    'How many apples are there if I have 5 apples and I eat 2 apples?'\n",
    "    'Compute the average temperature for Charlotte, NC for the past week. Be sure to use the appropriate tools.'\n",
    "    'What can you do with tools?'\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f'Prompt: {prompt}\\t\\n {agent.run(prompt)}\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Observations\n",
    "**Question:** What do you notice after adding `handle_parsing_errors = True`? What do you think that does under the hood?\n",
    "\n",
    "**Answer:** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting Everything Together: Hosting the Agent on Chainlit\n",
    "\n",
    "Now let's put everything together and build our agent in one cell! The cell below writes to a file called `simple_agent_app.py`. This file will be used to run our agent in the chainlit web interface. This cell will contain code that is indepedent from this notebook since it is written to a standalone python file, so we will be repeating some familiar steps, more practice! It is crucial that you run the cell below before you run the next cell. If you run the next cell first, you will get an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile simple_agent_app.py\n",
    "from chainlit.config import config\n",
    "import chainlit as cl\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.agents.initialize import initialize_agent\n",
    "from langchain.agents import load_tools\n",
    "from chainlit import on_message, on_chat_start\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import AgentType\n",
    "\n",
    "#TODO: load environment variables from your .env file\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "@cl.on_chat_start\n",
    "async def start():\n",
    "    # TODO: initialize LLM (we use ChatOpenAI because we'll later define a `chat` agent), \n",
    "    # Hint: How can you enable live generation of the agent's messages?\n",
    "    # BEGIN Writing\n",
    "    llm = ChatOpenAI(\n",
    "        temperature= 0,\n",
    "        # model_name='gpt-3.5-turbo',\n",
    "        model_name='meta-llama/Llama-2-70b-chat-hf',\n",
    "        max_tokens= 500,\n",
    "        streaming=True\n",
    "    )\n",
    "    #TODO: create a memory, load tools, and initialize an agent with the type `CHAT_CONVERSATIONAL_REACT_DESCRIPTION`\n",
    "    # Tip: Set `return_messages` to `True` in the memory so that the agent returns the messages it generates\n",
    "    conversational_memory = ConversationBufferMemory(\n",
    "        llm=llm,\n",
    "        input_key='input',\n",
    "        memory_key='chat_history',\n",
    "        max_token_limit=250,\n",
    "        return_messages=True,\n",
    "    )\n",
    "    #TODO: load the following tools ['llm-math', 'terminal', 'python_repl', 'serpapi']\n",
    "    tools = load_tools(['llm-math', 'terminal', 'python_repl', 'serpapi'], llm=llm)\n",
    "    #TODO: initialize agent\n",
    "    # Hint: There is a parameter that allows you to set the maximum number of iterations, \n",
    "        # this is useful to limit the number of messages the agent generates and hence $ spent on OAI credits.\n",
    "    # Hint: CHAT_CONVERSATIONAL_REACT_DESCRIPTION needs the memory to return its messages.\n",
    "    # Hint: How could you enable the agent to adapt to parsing issues it encounters?\n",
    "    agent = initialize_agent(\n",
    "        llm=llm,\n",
    "        memory=conversational_memory,\n",
    "        tools=tools,\n",
    "        verbose=True,\n",
    "        agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION, \n",
    "        handle_parsing_errors=True,\n",
    "        max_iterations=3,\n",
    "    )\n",
    "    # END Writing\n",
    "    # create agentexecutor\n",
    "    cl.user_session.set(\"agent\", agent)\n",
    "\n",
    "@cl.on_message\n",
    "async def on_message(message):\n",
    "    agent = cl.user_session.get(\"agent\")\n",
    "    response = await cl.make_async(agent.run)(\n",
    "        input=message, callbacks=[cl.LangchainCallbackHandler()]\n",
    "    )\n",
    "    await cl.Message(content=response).send()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's serve the chainlit web interface with our agent. Run the cell below to start the chainlit web interface. You should see a link to the web interface. Click on the link to open the web interface in a new tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note if this cell says the port is already in use, you can change the port number to something else e.g. 8001 and so on.\n",
    "!chainlit run simple_agent_app.py --port 8000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colab Setup\n",
    "If you are using Colab do not run the cell above, instead run this cell below. It will output a link and an ip-address. Use the link to access your chainlit UI application. It will first prompt you to enter the ip-address, enter the ip-address and click on connect. You should now be able to see the chainlit UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!npm install localtunnel\n",
    "!chainlit run /content/simple_agent_app.py &>/content/logs.txt & npx localtunnel --port 8000 & curl ipv4.icanhazip.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations and Insights\n",
    "Write any observations here. Try using the UI and having a proper conversation with your agent and list your observations for strengths and weaknesses. Inspect the prompt by clicking on the bug icon under the expanded `AgentExecutor` -> `LLMChain`.\n",
    "\n",
    "Food for thought:\n",
    "* Try using tools that rely on previous tool outputs for instance, `Are there any middle eastern restaurants around Charlotte?` then ask `What are the reviews for the first option?` Does it remember what the first option is?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langcourse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
