{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/taisazero/langchain-crashcourse/blob/main/notebooks/3-retrieval_aug_gen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Activity 2: Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain==0.0.294\n",
    "!pip install openai==0.28.1\n",
    "!pip install tiktoken\n",
    "!pip install python-dotenv \n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q lancedb==0.2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile .env\n",
    "# TODO: create a .env file with the following variables and then load them using dotenv.\n",
    "# * SERPAPI_API_KEY (Make one by signing up at https://serpapi.com/)\n",
    "# * OPENAI_API_KEY\n",
    "# Note: If you are using the Llama2 hosted model, set the OPENAI_API_KEY to the key provided \n",
    "# and also set OPENAI_API_BASE to the base url provided.\n",
    "OPENAI_API_KEY=\"YOUR_OPENAI_API_KEY\"\n",
    "# OPENAI_API_BASE=\"YOUR_OPENAI_API_BASE\" # Uncomment and use only if using Llama2 hosted model\n",
    "SERPAPI_API_KEY=\"YOUR_SERPAPI_API_KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lancedb\n",
    "from langchain.vectorstores import LanceDB\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import LanceDB\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "gdown.download_folder(\n",
    "    \"https://drive.google.com/drive/folders/1SkI0ttpMNTVHp6ear6cTLDooRmtqvmVo?usp=sharing\",\n",
    "      quiet=True,\n",
    "      output=\"./\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = lancedb.connect('.lancedb')\n",
    "table = db.open_table('hf_docs_oai')\n",
    "# TODO: Create an OpenAIEmbeddings object and a LanceDB vectorstore instantiated with the table and OpenAIEmbeddings.\n",
    "embedding_fn = OpenAIEmbeddings(chunk_size=200)\n",
    "vectorstore = LanceDB(table, embedding_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment: Retrieval-Augmented Generation\n",
    "\n",
    "TODO: Write a python snippet that creates an llm using the ChatOpenAI object with a temperature of 0, and prompt it with the question:\n",
    "\n",
    "> \"Can you describe PEFT from the transformers library?\"\n",
    "\n",
    "Store this question in a variable called `query`.\n",
    "\n",
    "Then, print the output of the llm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "#TODO: Create a RetrievalQA object with the ChatOpenAI model, the vectorstore's retriever, and a chain_type of \"stuff\".\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOpenAI(temperature=0, model='gpt-3.5-turbo-16k')\n",
    "    , chain_type=\"stuff\", retriever=vectorstore.as_retriever(search_kwargs=dict(k=5)),\n",
    "      verbose=True, return_source_documents = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['has a pretrained backbone, but it also uses the complete Transformer encoder-\\ndecoder architecture for object detection. The encoder learns image\\nrepresentations and combines them with object queries (each object query is a\\nlearned embedding that focuses on a region or object in an image) in the\\ndecoder. DETR predicts the bounding box coordinates and class label for each\\nobject query. Natural language processing Encoder BERT is an encoder-only\\nTransformer that randomly masks certain tokens in the input to avoid seeing\\nother tokens, which would allow it to “cheat”. The pretraining objective is to\\npredict the masked token based on the context. This allows BERT to fully use\\nthe left and right contexts to help it learn a deeper and richer\\nrepresentation of the inputs. However, there was still room for improvement in\\nBERT’s pretraining strategy. RoBERTa improved upon this by introducing a new\\npretraining recipe that includes training for longer and on larger batches,\\nrandomly masking tokens at each epoch instead of just once during\\npreprocessing, and removing the next-sentence prediction objective. The\\ndominant strategy to improve performance is to increase the model size. But\\ntraining large models is computationally expensive. One way to reduce\\ncomputational costs is using a smaller model like DistilBERT. DistilBERT uses\\nknowledge distillation - a compression technique - to create a smaller version\\nof BERT while keeping nearly all of its language understanding capabilities.\\nHowever, most Transformer models continued to trend towards more parameters,\\nleading to new models focused on improving training efficiency. ALBERT reduces\\nmemory consumption by lowering the number of parameters in two ways:\\nseparating the larger vocabulary embedding into two smaller matrices and\\nallowing layers to share parameters. DeBERTa added a disentangled attention\\nmechanism where the word and its position are separately encoded in two\\nvectors. The attention is computed from these separate vectors instead of a\\nsingle vector containing the word and position embeddings. Longformer also\\nfocused on making attention more efficient, especially for processing\\ndocuments with longer sequence lengths. It uses a combination of local\\nwindowed attention (attention only calculated from fixed window size around\\neach token) and global attention (only for specific task tokens like [CLS] for\\nclassification) to create a sparse attention matrix instead of a full\\nattention matrix. Decoder GPT-2 is a decoder-only Transformer that predicts\\nthe next word in the sequence. It masks tokens to the right so the model can’t\\n“cheat” by looking ahead. By pretraining on a massive body of text, GPT-2\\nbecame really good at generating text, even if the text is only sometimes\\naccurate or true. But GPT-2 lacked the bidirectional context from BERT’s\\npretraining, which made it unsuitable for certain tasks. XLNET combines the\\nbest of both BERT and GPT-2’s pretraining objectives by using a permutation\\nlanguage modeling objective (PLM) that allows it to learn bidirectionally.\\nAfter GPT-2, language models grew even bigger and are now known as large\\nlanguage models (LLMs). LLMs demonstrate few- or even zero-shot learning if\\npretrained on a large enough dataset. GPT-J is an LLM with 6B parameters and\\ntrained on 400B tokens. GPT-J was followed by OPT, a family of decoder-only\\nmodels, the largest of which is 175B and trained on 180B tokens. BLOOM was\\nreleased around the same time, and the largest model in the family has 176B\\nparameters and is trained on 366B tokens in 46 languages and 13 programming\\nlanguages. Encoder-decoder BART keeps the original Transformer architecture,\\nbut it modifies the pretraining objective with text infilling corruption,\\nwhere some text spans are replaced with a single mask token. The decoder\\npredicts the uncorrupted tokens (future tokens are masked) and uses the\\nencoder’s hidden states to help it. Pegasus is similar to BART, but Pegasus',\n",
       " 'Utilities for Trainer Utilities for Generation Utilities for Image Processors\\nUtilities for Audio processing General Utilities Utilities for Time Series\\nJoin the Hugging Face community and get access to the augmented documentation\\nexperience Collaborate on models, datasets and Spaces Faster examples with\\naccelerated inference Switch between documentation themes Sign Up to get\\nstarted BORT This model is in maintenance mode only, so we won’t accept any\\nnew PRs changing its code. If you run into any issues running this model,\\nplease reinstall the last version that supported this model: v4.30.0. You can\\ndo so by running the following command: pip install -U transformers==4.30.0.\\nOverview The BORT model was proposed in Optimal Subarchitecture Extraction for\\nBERT by Adrian de Wynter and Daniel J. Perry. It is an optimal subset of\\narchitectural parameters for the BERT, which the authors refer to as “Bort”.\\nThe abstract from the paper is the following: We extract an optimal subset of\\narchitectural parameters for the BERT architecture from Devlin et al. (2018)\\nby applying recent breakthroughs in algorithms for neural architecture search.\\nThis optimal subset, which we refer to as “Bort”, is demonstrably smaller,\\nhaving an effective (that is, not counting the embedding layer) size of 5.5%\\nthe original BERT-large architecture, and 16% of the net size. Bort is also\\nable to be pretrained in 288 GPU hours, which is 1.2% of the time required to\\npretrain the highest-performing BERT parametric architectural variant,\\nRoBERTa-large (Liu et al., 2019), and about 33% of that of the world-record,\\nin GPU hours, required to train BERT-large on the same hardware. It is also\\n7.9x faster on a CPU, as well as being better performing than other compressed\\nvariants of the architecture, and some of the non-compressed variants: it\\nobtains performance improvements of between 0.3% and 31%, absolute, with\\nrespect to BERT-large, on multiple public natural language understanding (NLU)\\nbenchmarks. Tips: BORT’s model architecture is based on BERT, so one can refer\\nto BERT’s documentation page for the model’s API as well as usage examples.\\nBORT uses the RoBERTa tokenizer instead of the BERT tokenizer, so one can\\nrefer to RoBERTa’s documentation page for the tokenizer’s API as well as usage\\nexamples. BORT requires a specific fine-tuning algorithm, called Agora , that\\nis sadly not open-sourced yet. It would be very useful for the community, if\\nsomeone tries to implement the algorithm to make BORT fine-tuning work. This\\nmodel was contributed by stefan-it. The original code can be found here.\\n←BLOOM ByT5→ BORT Overview',\n",
       " 'Wei Chang. VisualBERT is a neural network trained on a variety of (image,\\ntext) pairs. The abstract from the paper is the following: We propose\\nVisualBERT, a simple and flexible framework for modeling a broad range of\\nvision-and-language tasks. VisualBERT consists of a stack of Transformer\\nlayers that implicitly align elements of an input text and regions in an\\nassociated input image with self-attention. We further propose two visually-\\ngrounded language model objectives for pre-training VisualBERT on image\\ncaption data. Experiments on four vision-and-language tasks including VQA,\\nVCR, NLVR2, and Flickr30K show that VisualBERT outperforms or rivals with\\nstate-of-the-art models while being significantly simpler. Further analysis\\ndemonstrates that VisualBERT can ground elements of language to image regions\\nwithout any explicit supervision and is even sensitive to syntactic\\nrelationships, tracking, for example, associations between verbs and image\\nregions corresponding to their arguments. Tips: Most of the checkpoints\\nprovided work with the VisualBertForPreTraining configuration. Other\\ncheckpoints provided are the fine-tuned checkpoints for down-stream tasks -\\nVQA (‘visualbert-vqa’), VCR (‘visualbert-vcr’), NLVR2 (‘visualbert-nlvr2’).\\nHence, if you are not working on these downstream tasks, it is recommended\\nthat you use the pretrained checkpoints. For the VCR task, the authors use a\\nfine-tuned detector for generating visual embeddings, for all the checkpoints.\\nWe do not provide the detector and its weights as a part of the package, but\\nit will be available in the research projects, and the states can be loaded\\ndirectly into the detector provided. Usage VisualBERT is a multi-modal vision\\nand language model. It can be used for visual question answering, multiple\\nchoice, visual reasoning and region-to-phrase correspondence tasks. VisualBERT\\nuses a BERT-like transformer to prepare embeddings for image-text pairs. Both\\nthe text and visual features are then projected to a latent space with\\nidentical dimension. To feed images to the model, each image is passed through\\na pre-trained object detector and the regions and the bounding boxes are\\nextracted. The authors use the features generated after passing these regions\\nthrough a pre-trained CNN like ResNet as visual embeddings. They also add\\nabsolute position embeddings, and feed the resulting sequence of vectors to a\\nstandard BERT model. The text input is concatenated in the front of the visual\\nembeddings in the embedding layer, and is expected to be bound by [CLS] and a\\n[SEP] tokens, as in BERT. The segment IDs must also be set appropriately for\\nthe textual and visual parts. The BertTokenizer is used to encode the text. A\\ncustom detector/image processor must be used to get the visual embeddings. The\\nfollowing example notebooks show how to use VisualBERT with Detectron-like\\nmodels: VisualBERT VQA demo notebook : This notebook contains an example on\\nVisualBERT VQA. Generate Embeddings for VisualBERT (Colab Notebook) : This\\nnotebook contains an example on how to generate visual embeddings. The\\nfollowing example shows how to get the last hidden state using\\nVisualBertModel: Copied >>> import torch >>> from transformers import\\nBertTokenizer, VisualBertModel >>> model =\\nVisualBertModel.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\") >>>\\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\") >>> inputs =\\ntokenizer(\"What is the man eating?\", return_tensors=\"pt\") >>> # this is a\\ncustom function that returns the visual embeddings given the image path >>>\\nvisual_embeds = get_visual_embeddings(image_path) >>> visual_token_type_ids =\\ntorch.ones(visual_embeds.shape[:-1], dtype=torch.long) >>>\\nvisual_attention_mask = torch.ones(visual_embeds.shape[:-1],\\ndtype=torch.float) >>> inputs.update( ... { ... \"visual_embeds\":\\nvisual_embeds, ... \"visual_token_type_ids\": visual_token_type_ids, ...\\n\"visual_attention_mask\": visual_attention_mask, ... } ... ) >>> outputs =',\n",
       " 'of Deep Bidirectional Transformers for Language Understanding by Jacob Devlin,\\nMing-Wei Chang, Kenton Lee and Kristina Toutanova. BERT For Sequence\\nGeneration (from Google) released with the paper Leveraging Pre-trained\\nCheckpoints for Sequence Generation Tasks by Sascha Rothe, Shashi Narayan,\\nAliaksei Severyn. BERTweet (from VinAI Research) released with the paper\\nBERTweet: A pre-trained language model for English Tweets by Dat Quoc Nguyen,\\nThanh Vu and Anh Tuan Nguyen. BigBird-Pegasus (from Google Research) released\\nwith the paper Big Bird: Transformers for Longer Sequences by Manzil Zaheer,\\nGuru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago\\nOntanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed. BigBird-\\nRoBERTa (from Google Research) released with the paper Big Bird: Transformers\\nfor Longer Sequences by Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua\\nAinslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan\\nWang, Li Yang, Amr Ahmed. BioGpt (from Microsoft Research AI4Science) released\\nwith the paper BioGPT: generative pre-trained transformer for biomedical text\\ngeneration and mining by Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng\\nZhang, Hoifung Poon and Tie-Yan Liu. BiT (from Google AI) released with the\\npaper Big Transfer (BiT): General Visual Representation Learning by Alexander\\nKolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain\\nGelly, Neil Houlsby. Blenderbot (from Facebook) released with the paper\\nRecipes for building an open-domain chatbot by Stephen Roller, Emily Dinan,\\nNaman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt\\nShuster, Eric M. Smith, Y-Lan Boureau, Jason Weston. BlenderbotSmall (from\\nFacebook) released with the paper Recipes for building an open-domain chatbot\\nby Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan\\nLiu, Jing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason\\nWeston. BLIP (from Salesforce) released with the paper BLIP: Bootstrapping\\nLanguage-Image Pre-training for Unified Vision-Language Understanding and\\nGeneration by Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi. BLIP-2 (from\\nSalesforce) released with the paper BLIP-2: Bootstrapping Language-Image Pre-\\ntraining with Frozen Image Encoders and Large Language Models by Junnan Li,\\nDongxu Li, Silvio Savarese, Steven Hoi. BLOOM (from BigScience workshop)\\nreleased by the BigScience Workshop. BORT (from Alexa) released with the paper\\nOptimal Subarchitecture Extraction For BERT by Adrian de Wynter and Daniel J.\\nPerry. BridgeTower (from Harbin Institute of Technology/Microsoft Research\\nAsia/Intel Labs) released with the paper BridgeTower: Building Bridges Between\\nEncoders in Vision-Language Representation Learning by Xiao Xu, Chenfei Wu,\\nShachar Rosenman, Vasudev Lal, Wanxiang Che, Nan Duan. ByT5 (from Google\\nResearch) released with the paper ByT5: Towards a token-free future with pre-\\ntrained byte-to-byte models by Linting Xue, Aditya Barua, Noah Constant, Rami\\nAl-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel. CamemBERT\\n(from Inria/Facebook/Sorbonne) released with the paper CamemBERT: a Tasty\\nFrench Language Model by Louis Martin, Benjamin Muller, Pedro Javier Ortiz\\nSuárez*, Yoann Dupont, Laurent Romary, Éric Villemonte de la Clergerie, Djamé\\nSeddah and Benoît Sagot. CANINE (from Google Research) released with the paper\\nCANINE: Pre-training an Efficient Tokenization-Free Encoder for Language\\nRepresentation by Jonathan H. Clark, Dan Garrette, Iulia Turc, John Wieting.\\nChinese-CLIP (from OFA-Sys) released with the paper Chinese CLIP: Contrastive\\nVision-Language Pretraining in Chinese by An Yang, Junshu Pan, Junyang Lin,\\nRui Men, Yichang Zhang, Jingren Zhou, Chang Zhou. CLAP (from LAION-AI)\\nreleased with the paper Large-scale Contrastive Language-Audio Pretraining\\nwith Feature Fusion and Keyword-to-Caption Augmentation by Yusong Wu, Ke Chen,']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def query_database(query, return_text = False):\n",
    "  ## YOUR CODE HERE\n",
    "  response = vectorstore.similarity_search(query)\n",
    "  if return_text:\n",
    "    return [doc.page_content for doc in response]\n",
    "  \n",
    "  return response\n",
    "\n",
    "query_database(\"Describe BERT's parameters\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# TODO: Ask a question about the transformers library.\n",
    "query = \"Can you describe PEFT from the transformers library?\"\n",
    "answer = qa({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(answer['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment: Vanilla Generation\n",
    "\n",
    "TODO: Write a python snippet that creates an llm using the ChatOpenAI object with a temperature of 0, and prompt it with `query`. Then, print the output of the llm. `query` is a string that contains the query you want to prompt the llm with and it should ask the language model a question related to the Huggingface library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write your code here.\n",
    "llm=ChatOpenAI(temperature=0, model='gpt-3.5-turbo-16k')\n",
    "print(llm.predict(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations and Insights\n",
    "Write your observations and insights here. What do you think about the quality and correctness of the generated text from the two experiments?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Prompts\n",
    "Now try querying the vanilla llm and the `RetrievalQA` chain with different questions related to the [**Huggingface** libraries](https://huggingface.co/docs/transformers/index). What do you observe? You are given two prompts below to try out first. Add your own prompts in new cells and try them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Can you give me a crash course on using the HF Trainer to train a GPT-style model?\"\n",
    "answer = qa({\"query\": query})\n",
    "print(answer['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Can you give me a crash course on using the HF Trainer to train a GPT-style model? Provide code snippets for each step.\"\n",
    "answer = qa({\"query\": query})\n",
    "print(answer['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Can you give me a crash course on using the HF Trainer to train a GPT-style model?\"\n",
    "print(llm.predict(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Can you give me a crash course on using the HF Trainer to train a GPT-style model? Provide code snippets for each step.\"\n",
    "print(llm.predict(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Increasing the Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=ChatOpenAI(temperature=0.8, model='gpt-3.5-turbo-16k')\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, chain_type=\"stuff\", retriever=vectorstore.as_retriever(search_kwargs=dict(k=5)),\n",
    "      verbose=True, return_source_documents = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Add your queries/questions here.\n",
    "query = \"Can you give me a crash course on using the HF Trainer to train a GPT-style model? Provide code snippets for each step.\"\n",
    "answer = qa({\"query\": query})\n",
    "print(answer['result'])\n",
    "llm.predict(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try querying the vanilla llm and the `RetrievalQA` chain with the same questions after we increased the temperature. What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Add the queries here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations and Insights\n",
    "Write your observations and insights here.\n",
    "Food for thought:\n",
    "* What was the impact of the instruction/prompt on the quality of the generated text?\n",
    "* What did you observe when increasing the temperature of the llm in the retrieval-augmented generation setting?\n",
    "* What do you think are the advantages and disadvantages of using retrieval-augmented generation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Internet Search Engines to Augment Retrieval\n",
    "\n",
    "Hypothesis: This will be more useful than the previous method for questions that require a lot of background knowledge from multiple sources, such as \"what is the best way to train a language model?\", \"how do I ... do in HF\" and \"crash course\" requests. This approach will allow us to retrieve more relevant documents from the internet and cite them in our answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q google-api-python-client\n",
    "!pip install -q chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "1. To create an API key: Go to https://console.cloud.google.com/welcome/new and sign in/up using your personal Google account. \n",
    "   - Navigate to the APIs & Services→Credentials panel in Cloud Console. \n",
    "   - Select Create credentials, then select API key from the drop-down menu. \n",
    "   - The API key created dialog box displays your newly created key. \n",
    "   - You now have an API_KEY. Store in it in your `.env` file as `GOOGLE_API_KEY`.\n",
    "\n",
    "2. Setup Custom Search Engine so you can search the entire web \n",
    "   - Create a custom search engine in this [link](http://www.google.com/cse/). \n",
    "   -  In Sites to search, select search the entire web. Select also enable SafeSearch and fill in your search engine name (it can be anything).\n",
    "   - Click on customize.\n",
    "   - Under Search engine ID you’ll find the search-engine-ID. Copy that and add it to your `.env` file as `GOOGLE_CSE_ID`.\n",
    "\n",
    "3. Enable the Custom Search API \n",
    "   - Go to this [URL](https://console.cloud.google.com/apis/library/customsearch.googleapis.com) & click on Enable.\n",
    "   - Alternatively, navigate to the APIs & Services→Dashboard panel in Cloud Console. \n",
    "   - Click Enable APIs and Services. \n",
    "   - Search for Custom Search API and click on it. \n",
    "   - Click Enable. \n",
    "\n",
    "**Note**: Adapted from [GoogleSearchAPIWrapper Docs](https://api.python.langchain.com/en/latest/utilities/langchain.utilities.google_search.GoogleSearchAPIWrapper.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Re-load the environment variables again\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.web_research import WebResearchRetriever\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.utilities import GoogleSearchAPIWrapper\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Vectorstore\n",
    "vectorstore = Chroma(embedding_function=OpenAIEmbeddings(), persist_directory=\"./chromadb_oai\")\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(temperature=0, model='gpt-3.5-turbo-16k', max_tokens=512)\n",
    "\n",
    "search = GoogleSearchAPIWrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "web_research_retriever = WebResearchRetriever.from_llm(\n",
    "    vectorstore=vectorstore,\n",
    "    llm=llm, \n",
    "    search=search,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "query = \"How can I finetune a Llama2 7b model on Google Colab?\"\n",
    "qa_chain = RetrievalQAWithSourcesChain.from_chain_type(llm, retriever=web_research_retriever)\n",
    "result = qa_chain({\"question\": query})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "query = \"What are some recent news articles about OpenAI?\"\n",
    "qa_chain = RetrievalQAWithSourcesChain.from_chain_type(llm, retriever=web_research_retriever)\n",
    "result = qa_chain({\"question\": query})\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO for Erfan: Try adapting from https://blog.langchain.dev/building-chat-langchain-2/ to increase robustness. Can then do a VSCode Extension. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations and Insights\n",
    "Write any observations here.\n",
    "Food for thought:\n",
    "* Compare and contrast this approach with the previous huggingface question answering chain. What are the limitations of each approach? Can the previous approach search the entire web? Can this approach generate precise answers?\n",
    "* If we want to be able to retrieve more relevant documents from the internet but also generate complete answers to the user's question without having the user to read the retrieved documents. What are somethings we can do to make this approach do so? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ret_aug_app.py\n",
    "from chainlit.config import config\n",
    "import chainlit as cl\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.agents.initialize import initialize_agent\n",
    "from langchain.agents import load_tools\n",
    "from chainlit import on_message, on_chat_start\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import AgentType\n",
    "from langchain.tools import Tool\n",
    "import lancedb\n",
    "from langchain.vectorstores import LanceDB\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import LanceDB\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "\n",
    "#TODO: load environment variables from your .env file\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "@cl.on_chat_start\n",
    "async def start():\n",
    "    # TODO: initialize LLM (we use ChatOpenAI because we'll later define a `chat` agent), \n",
    "    # Hint: How can you enable live generation of the agent's messages?\n",
    "    # BEGIN Writing\n",
    "    llm = ChatOpenAI(\n",
    "        temperature= 0,\n",
    "        model_name='gpt-3.5-turbo-16k',\n",
    "        max_tokens= 750,\n",
    "        streaming=True\n",
    "    )\n",
    "    #TODO: create a memory, load tools, and initialize an agent with the type `CHAT_CONVERSATIONAL_REACT_DESCRIPTION`\n",
    "    # Tip: Set `return_messages` to `True` in the memory so that the agent returns the messages it generates\n",
    "    conversational_memory = ConversationBufferMemory(\n",
    "        llm=llm,\n",
    "        input_key='input',\n",
    "        memory_key='chat_history',\n",
    "        max_token_limit=250,\n",
    "        return_messages=True,\n",
    "    )\n",
    "    #TODO: load the following tools ['llm-math', 'terminal', 'python_repl', 'serpapi']\n",
    "    tools = load_tools(['llm-math', 'terminal', 'python_repl', 'serpapi'], llm=llm)\n",
    "    #TODO: connect to `.lancedb` and open the `hf_docs` table, then create a LanceDB vectorstore with the table \n",
    "    # and the OpenAIEmbeddings object with chunk_size 200.\n",
    "    db = lancedb.connect('.lancedb')\n",
    "    table = db.open_table('hf_docs')\n",
    "    embedding_fn = OpenAIEmbeddings(chunk_size=200)\n",
    "    vectorstore = LanceDB(table, embedding_fn)\n",
    "    #TODO: initialize your retrievalQA chain using the LanceDB `vectorstore` and the `llm` you initialized above\n",
    "    # pass in chain_type of \"stuff\", and retrieve the top 5 documents.\n",
    "    qa = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\", retriever=vectorstore.as_retriever(search_kwargs=dict(k=5)),\n",
    "        verbose=True\n",
    "    )\n",
    "    # TODO: Create a tool that uses the `qa` chain we just initialized\n",
    "    # Hint: Use the Tool.from_function method\n",
    "    hf_docs_qa_tool = Tool.from_function(\n",
    "        qa.run, name=\"huggingface documentation search\",\n",
    "        description=\"Use when you need to search the Hugging Face documentation for an answer to a question.\"\n",
    "    )\n",
    "    # We add the tool to the tools list\n",
    "    tools.append(hf_docs_qa_tool)\n",
    "    #TODO: initialize agent\n",
    "    # Hint: There is a parameter that allows you to set the maximum number of iterations, \n",
    "        # this is useful to limit the number of messages the agent generates and hence $ spent on OAI credits.\n",
    "    # Hint: CHAT_CONVERSATIONAL_REACT_DESCRIPTION needs the memory to return its messages.\n",
    "    # Hint: How could you enable the agent to adapt to parsing issues it encounters?\n",
    "    agent = initialize_agent(\n",
    "        llm=llm,\n",
    "        memory=conversational_memory,\n",
    "        tools=tools,\n",
    "        verbose=True,\n",
    "        agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION, \n",
    "        handle_parsing_errors=True,\n",
    "        max_iterations=3,\n",
    "    )\n",
    "    \n",
    "    # END Writing\n",
    "    # create agentexecutor\n",
    "    cl.user_session.set(\"agent\", agent)\n",
    "\n",
    "@cl.on_message\n",
    "async def on_message(message):\n",
    "    agent = cl.user_session.get(\"agent\")\n",
    "    response = await cl.make_async(agent.run)(\n",
    "        input=message, callbacks=[cl.LangchainCallbackHandler()]\n",
    "    )\n",
    "    await cl.Message(content=response).send()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's serve the chainlit web interface with our agent. Run the cell below to start the chainlit web interface. You should see a link to the web interface. Click on the link to open the web interface in a new tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note if this cell says the port is already in use, you can change the port number to something else e.g. 8001 and so on.\n",
    "!chainlit run ret_aug_app.py --port 8001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colab Setup\n",
    "If you are using Colab do not run the cell above, instead run this cell below. It will output a link and an ip-address. Use the link to access your chainlit UI application. It will first prompt you to enter the ip-address, enter the ip-address and click on connect. You should now be able to see the chainlit UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!npm install localtunnel\n",
    "!chainlit run /content/ret_aug_app.py &>/content/logs.txt & npx localtunnel --port 8000 & curl ipv4.icanhazip.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Tasks\n",
    "If you finish early, you can try to complete the following tasks:\n",
    "* Insert a prompt to the RetrievalQAChain that instructs the model to formulate search engine optimal questions given the user query. Use the generated questions to query the search engine and retrieve documents. Try different prompts and instructions.\n",
    "* Add your best RetrievalQAChain to your agent as a tool from the last lab activity. Hint: Look into defining custom tools [langchain documentation](https://python.langchain.com/docs/modules/agents/tools/custom_tools).\n",
    "* Re-run the experiments with Llama2, compare and contrast the outputs based on correctness, helpfulness and conciseness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "* https://lancedb.github.io/lancedb/notebooks/code_qa_bot/\n",
    "* https://github.com/langchain-ai/chat-langchain/tree/master"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langcourse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
