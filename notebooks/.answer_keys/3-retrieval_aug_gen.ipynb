{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/taisazero/langchain-crashcourse/blob/main/notebooks/3-retrieval_aug_gen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Activity 2: Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q lancedb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lancedb\n",
    "from langchain.vectorstores import LanceDB\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import LanceDB\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "gdown.download_folder(\n",
    "    \"https://drive.google.com/drive/folders/1SkI0ttpMNTVHp6ear6cTLDooRmtqvmVo?usp=sharing\",\n",
    "      quiet=True,\n",
    "      output=\"./\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = lancedb.connect('.lancedb')\n",
    "table = db.open_table('hf_docs')\n",
    "# TODO: Create an OpenAIEmbeddings object and a LanceDB vectorstore instantiated with the table and OpenAIEmbeddings.\n",
    "embedding_fn = OpenAIEmbeddings(chunk_size=200)\n",
    "vectorstore = LanceDB(table, embedding_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment: Retrieval-Augmented Generation\n",
    "\n",
    "TODO: Write a python snippet that creates an llm using the ChatOpenAI object with a temperature of 0, and prompt it with the question:\n",
    "\n",
    "> \"Can you describe PEFT from the transformers library?\"\n",
    "\n",
    "Store this question in a variable called `query`.\n",
    "\n",
    "Then, print the output of the llm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "#TODO: Create a RetrievalQA object with the ChatOpenAI model, the vectorstore's retriever, and a chain_type of \"stuff\".\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOpenAI(temperature=0, model='gpt-3.5-turbo-16k')\n",
    "    , chain_type=\"stuff\", retriever=vectorstore.as_retriever(search_kwargs=dict(k=5)),\n",
    "      verbose=True, return_source_documents = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Ask a question about the transformers library.\n",
    "query = \"Can you describe PEFT from the transformers library?\"\n",
    "answer = qa({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(answer['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment: Vanilla Generation\n",
    "\n",
    "TODO: Write a python snippet that creates an llm using the ChatOpenAI object with a temperature of 0, and prompt it with `query`. Then, print the output of the llm. `query` is a string that contains the query you want to prompt the llm with and it should ask the language model a question related to the Huggingface library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write your code here.\n",
    "llm=ChatOpenAI(temperature=0, model='gpt-3.5-turbo-16k')\n",
    "print(llm.predict(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations and Insights\n",
    "Write your observations and insights here. What do you think about the quality and correctness of the generated text from the two experiments?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Prompts\n",
    "Now try querying the vanilla llm and the `RetrievalQA` chain with different questions related to the [**Huggingface** libraries](https://huggingface.co/docs/transformers/index). What do you observe? You are given two prompts below to try out first. Add your own prompts in new cells and try them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Can you give me a crash course on using the HF Trainer to train a GPT-style model?\"\n",
    "answer = qa({\"query\": query})\n",
    "print(answer['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Can you give me a crash course on using the HF Trainer to train a GPT-style model? Provide code snippets for each step.\"\n",
    "answer = qa({\"query\": query})\n",
    "print(answer['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Can you give me a crash course on using the HF Trainer to train a GPT-style model?\"\n",
    "print(llm.predict(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Can you give me a crash course on using the HF Trainer to train a GPT-style model? Provide code snippets for each step.\"\n",
    "print(llm.predict(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Increasing the Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=ChatOpenAI(temperature=0.8, model='gpt-3.5-turbo-16k')\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, chain_type=\"stuff\", retriever=vectorstore.as_retriever(search_kwargs=dict(k=5)),\n",
    "      verbose=True, return_source_documents = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Add your queries/questions here.\n",
    "query = \"Can you give me a crash course on using the HF Trainer to train a GPT-style model? Provide code snippets for each step.\"\n",
    "answer = qa({\"query\": query})\n",
    "print(answer['result'])\n",
    "llm.predict(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try querying the vanilla llm and the `RetrievalQA` chain with the same questions after we increased the temperature. What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Add the queries here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations and Insights\n",
    "Write your observations and insights here.\n",
    "Food for thought:\n",
    "* What was the impact of the instruction/prompt on the quality of the generated text?\n",
    "* What did you observe when increasing the temperature of the llm in the retrieval-augmented generation setting?\n",
    "* What do you think are the advantages and disadvantages of using retrieval-augmented generation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Internet Search Engines to Augment Retrieval\n",
    "\n",
    "Hypothesis: This will be more useful than the previous method for questions that require a lot of background knowledge from multiple sources, such as \"what is the best way to train a language model?\", \"how do I ... do in HF\" and \"crash course\" requests. This approach will allow us to retrieve more relevant documents from the internet and cite them in our answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q google-api-python-client\n",
    "!pip install -q chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "1. To create an API key: Go to https://console.cloud.google.com/welcome/new and sign in/up using your personal Google account. \n",
    "   - Navigate to the APIs & Services→Credentials panel in Cloud Console. \n",
    "   - Select Create credentials, then select API key from the drop-down menu. \n",
    "   - The API key created dialog box displays your newly created key. \n",
    "   - You now have an API_KEY. Store in it in your `.env` file as `GOOGLE_API_KEY`.\n",
    "\n",
    "2. Setup Custom Search Engine so you can search the entire web \n",
    "   - Create a custom search engine in this [link](http://www.google.com/cse/). \n",
    "   -  In Sites to search, select search the entire web. Select also enable SafeSearch and fill in your search engine name (it can be anything).\n",
    "   - Click on customize.\n",
    "   - Under Search engine ID you’ll find the search-engine-ID. Copy that and add it to your `.env` file as `GOOGLE_CSE_ID`.\n",
    "\n",
    "3. Enable the Custom Search API \n",
    "   - Go to this [URL](https://console.cloud.google.com/apis/library/customsearch.googleapis.com) & click on Enable.\n",
    "   - Alternatively, navigate to the APIs & Services→Dashboard panel in Cloud Console. \n",
    "   - Click Enable APIs and Services. \n",
    "   - Search for Custom Search API and click on it. \n",
    "   - Click Enable. \n",
    "\n",
    "**Note**: Adapted from [GoogleSearchAPIWrapper Docs](https://api.python.langchain.com/en/latest/utilities/langchain.utilities.google_search.GoogleSearchAPIWrapper.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Re-load the environment variables again\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.web_research import WebResearchRetriever\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.utilities import GoogleSearchAPIWrapper\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Vectorstore\n",
    "vectorstore = Chroma(embedding_function=OpenAIEmbeddings(), persist_directory=\"./chromadb_oai\")\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(temperature=0, model='gpt-3.5-turbo-16k', max_tokens=512)\n",
    "\n",
    "search = GoogleSearchAPIWrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "web_research_retriever = WebResearchRetriever.from_llm(\n",
    "    vectorstore=vectorstore,\n",
    "    llm=llm, \n",
    "    search=search,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "query = \"How can I finetune a Llama2 7b model on Google Colab?\"\n",
    "qa_chain = RetrievalQAWithSourcesChain.from_chain_type(llm, retriever=web_research_retriever)\n",
    "result = qa_chain({\"question\": query})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "query = \"What are some recent news articles about OpenAI?\"\n",
    "qa_chain = RetrievalQAWithSourcesChain.from_chain_type(llm, retriever=web_research_retriever)\n",
    "result = qa_chain({\"question\": query})\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations and Insights\n",
    "Write any observations here.\n",
    "Food for thought:\n",
    "* Compare and contrast this approach with the previous huggingface question answering chain. What are the limitations of each approach? Can the previous approach search the entire web? Can this approach generate precise answers?\n",
    "* If we want to be able to retrieve more relevant documents from the internet but also generate complete answers to the user's question without having the user to read the retrieved documents. What are somethings we can do to make this approach do so? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ret_aug_app.py\n",
    "from chainlit.config import config\n",
    "import chainlit as cl\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.agents.initialize import initialize_agent\n",
    "from langchain.agents import load_tools\n",
    "from chainlit import on_message, on_chat_start\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import AgentType\n",
    "from langchain.tools import Tool\n",
    "import lancedb\n",
    "from langchain.vectorstores import LanceDB\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import LanceDB\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "\n",
    "#TODO: load environment variables from your .env file\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "@cl.on_chat_start\n",
    "async def start():\n",
    "    # TODO: initialize LLM (we use ChatOpenAI because we'll later define a `chat` agent), \n",
    "    # Hint: How can you enable live generation of the agent's messages?\n",
    "    # BEGIN Writing\n",
    "    llm = ChatOpenAI(\n",
    "        temperature= 0,\n",
    "        model_name='gpt-3.5-turbo-16k',\n",
    "        max_tokens= 750,\n",
    "        streaming=True\n",
    "    )\n",
    "    #TODO: create a memory, load tools, and initialize an agent with the type `CHAT_CONVERSATIONAL_REACT_DESCRIPTION`\n",
    "    # Tip: Set `return_messages` to `True` in the memory so that the agent returns the messages it generates\n",
    "    conversational_memory = ConversationBufferMemory(\n",
    "        llm=llm,\n",
    "        input_key='input',\n",
    "        memory_key='chat_history',\n",
    "        max_token_limit=250,\n",
    "        return_messages=True,\n",
    "    )\n",
    "    #TODO: load the following tools ['llm-math', 'terminal', 'python_repl', 'serpapi']\n",
    "    tools = load_tools(['llm-math', 'terminal', 'python_repl', 'serpapi'], llm=llm)\n",
    "    #TODO: connect to `.lancedb` and open the `hf_docs` table, then create a LanceDB vectorstore with the table \n",
    "    # and the OpenAIEmbeddings object with chunk_size 200.\n",
    "    db = lancedb.connect('.lancedb')\n",
    "    table = db.open_table('hf_docs')\n",
    "    embedding_fn = OpenAIEmbeddings(chunk_size=200)\n",
    "    vectorstore = LanceDB(table, embedding_fn)\n",
    "    #TODO: initialize your retrievalQA chain using the LanceDB `vectorstore` and the `llm` you initialized above\n",
    "    # pass in chain_type of \"stuff\", and retrieve the top 5 documents.\n",
    "    qa = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\", retriever=vectorstore.as_retriever(search_kwargs=dict(k=5)),\n",
    "        verbose=True\n",
    "    )\n",
    "    # TODO: Create a tool that uses the `qa` chain we just initialized\n",
    "    # Hint: Use the Tool.from_function method\n",
    "    hf_docs_qa_tool = Tool.from_function(\n",
    "        qa.run, name=\"huggingface documentation search\",\n",
    "        description=\"Use when you need to search the Hugging Face documentation for an answer to a question.\"\n",
    "    )\n",
    "    # We add the tool to the tools list\n",
    "    tools.append(hf_docs_qa_tool)\n",
    "    #TODO: initialize agent\n",
    "    # Hint: There is a parameter that allows you to set the maximum number of iterations, \n",
    "        # this is useful to limit the number of messages the agent generates and hence $ spent on OAI credits.\n",
    "    # Hint: CHAT_CONVERSATIONAL_REACT_DESCRIPTION needs the memory to return its messages.\n",
    "    # Hint: How could you enable the agent to adapt to parsing issues it encounters?\n",
    "    agent = initialize_agent(\n",
    "        llm=llm,\n",
    "        memory=conversational_memory,\n",
    "        tools=tools,\n",
    "        verbose=True,\n",
    "        agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION, \n",
    "        handle_parsing_errors=True,\n",
    "        max_iterations=3,\n",
    "    )\n",
    "    \n",
    "    # END Writing\n",
    "    # create agentexecutor\n",
    "    cl.user_session.set(\"agent\", agent)\n",
    "\n",
    "@cl.on_message\n",
    "async def on_message(message):\n",
    "    agent = cl.user_session.get(\"agent\")\n",
    "    response = await cl.make_async(agent.run)(\n",
    "        input=message, callbacks=[cl.LangchainCallbackHandler()]\n",
    "    )\n",
    "    await cl.Message(content=response).send()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's serve the chainlit web interface with our agent. Run the cell below to start the chainlit web interface. You should see a link to the web interface. Click on the link to open the web interface in a new tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note if this cell says the port is already in use, you can change the port number to something else e.g. 8001 and so on.\n",
    "!chainlit run ret_aug_app.py --port 8001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colab Setup\n",
    "If you are using Colab do not run the cell above, instead run this cell below. It will output a link and an ip-address. Use the link to access your chainlit UI application. It will first prompt you to enter the ip-address, enter the ip-address and click on connect. You should now be able to see the chainlit UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!npm install localtunnel\n",
    "!chainlit run /content/ret_aug_app.py &>/content/logs.txt & npx localtunnel --port 8000 & curl ipv4.icanhazip.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Tasks\n",
    "If you finish early, you can try to complete the following tasks:\n",
    "* Insert a prompt to the RetrievalQAChain that instructs the model to formulate search engine optimal questions given the user query. Use the generated questions to query the search engine and retrieve documents. Try different prompts and instructions.\n",
    "* Add your best RetrievalQAChain to your agent as a tool from the last lab activity. Hint: Look into defining custom tools [langchain documentation](https://python.langchain.com/docs/modules/agents/tools/custom_tools).\n",
    "* Re-run the experiments with Llama2, compare and contrast the outputs based on correctness, helpfulness and conciseness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "* https://lancedb.github.io/lancedb/notebooks/code_qa_bot/\n",
    "* https://github.com/langchain-ai/chat-langchain/tree/master"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langcourse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
