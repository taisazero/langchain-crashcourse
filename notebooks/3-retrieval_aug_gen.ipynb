{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lancedb\n",
    "from langchain.vectorstores import LanceDB\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import LanceDB\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gdown\n",
      "  Downloading gdown-4.7.1-py3-none-any.whl (15 kB)\n",
      "Collecting filelock (from gdown)\n",
      "  Obtaining dependency information for filelock from https://files.pythonhosted.org/packages/5e/5d/97afbafd9d584ff1b45fcb354a479a3609bd97f912f8f1f6c563cb1fae21/filelock-3.12.4-py3-none-any.whl.metadata\n",
      "  Downloading filelock-3.12.4-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\erfan\\miniconda3\\envs\\langcourse\\lib\\site-packages (from gdown) (2.31.0)\n",
      "Requirement already satisfied: six in c:\\users\\erfan\\miniconda3\\envs\\langcourse\\lib\\site-packages (from gdown) (1.16.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\erfan\\miniconda3\\envs\\langcourse\\lib\\site-packages (from gdown) (4.66.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\erfan\\miniconda3\\envs\\langcourse\\lib\\site-packages (from gdown) (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\erfan\\miniconda3\\envs\\langcourse\\lib\\site-packages (from beautifulsoup4->gdown) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\erfan\\miniconda3\\envs\\langcourse\\lib\\site-packages (from requests[socks]->gdown) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\erfan\\miniconda3\\envs\\langcourse\\lib\\site-packages (from requests[socks]->gdown) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\erfan\\miniconda3\\envs\\langcourse\\lib\\site-packages (from requests[socks]->gdown) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\erfan\\miniconda3\\envs\\langcourse\\lib\\site-packages (from requests[socks]->gdown) (2023.7.22)\n",
      "Collecting PySocks!=1.5.7,>=1.5.6 (from requests[socks]->gdown)\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\erfan\\miniconda3\\envs\\langcourse\\lib\\site-packages (from tqdm->gdown) (0.4.6)\n",
      "Downloading filelock-3.12.4-py3-none-any.whl (11 kB)\n",
      "Installing collected packages: PySocks, filelock, gdown\n",
      "Successfully installed PySocks-1.7.1 filelock-3.12.4 gdown-4.7.1\n"
     ]
    }
   ],
   "source": [
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "gdown.download_folder(\n",
    "    \"https://drive.google.com/drive/folders/1SkI0ttpMNTVHp6ear6cTLDooRmtqvmVo?usp=sharing\",\n",
    "      quiet=True,\n",
    "      output=\"./\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = lancedb.connect('../.lancedb')\n",
    "table = db.open_table('hf_docs')\n",
    "# TODO: Create an OpenAIEmbeddings object and a LanceDB vectorstore instantiated with the table and OpenAIEmbeddings.\n",
    "embedding_fn = OpenAIEmbeddings(chunk_size=200)\n",
    "vectorstore = LanceDB(table, embedding_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "#TODO: Create a RetrievalQA object with the ChatOpenAI model, the vectorstore's retriever, and a chain_type of \"stuff\".\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOpenAI(temperature=0, model='gpt-3.5-turbo-16k')\n",
    "    , chain_type=\"stuff\", retriever=vectorstore.as_retriever(search_kwargs=dict(k=5)),\n",
    "      verbose=True, return_source_documents = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "query = \"Can you describe PEFT from the transformers library?\"\n",
    "answer = qa({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Can you describe computer vision in the transformers library?',\n",
       " 'result': 'In the Transformers library, computer vision tasks are supported through various models and pipelines. The library provides pre-trained models for image classification, object detection, and image segmentation tasks. These models are based on Transformer architectures, which have been proven to be effective in natural language processing tasks and have been adapted for computer vision tasks as well.\\n\\nFor image classification, the library offers models like Vision Transformer (ViT) and Convolutional Vision Transformer (CvT). These models can take an image as input and classify it into different categories or labels.\\n\\nFor object detection, the library provides models like DETR (DEtection TRansformer) and YOLO (You Only Look Once). These models can detect and localize objects within an image, providing bounding box coordinates and class labels for each detected object.\\n\\nFor image segmentation, the library offers models like UNet and DeepLabV3. These models can segment an image into different regions or objects, assigning a label to each pixel.\\n\\nIn addition to these models, the Transformers library also provides pipelines that make it easy to perform computer vision tasks with just a few lines of code. These pipelines handle the preprocessing of images, running them through the models, and providing the output in a convenient format.\\n\\nOverall, the Transformers library provides a range of models and tools to solve computer vision tasks, allowing users to leverage the power of Transformer architectures for image analysis and understanding.',\n",
       " 'source_documents': [Document(page_content='What 🤗 Transformers can do Hugging Face Models Datasets Spaces Docs Solutions\\nPricing Log In Sign Up Transformers documentation What 🤗 Transformers can do\\nTransformers Search documentation\\nmainv4.33.2v4.32.1v4.31.0v4.30.0v4.29.1v4.28.1v4.27.2v4.26.1v4.25.1v4.24.0v4.23.1v4.22.2v4.21.3v4.20.1v4.19.4v4.18.0v4.17.0v4.16.2v4.15.0v4.14.1v4.13.0v4.12.5v4.11.3v4.10.1v4.9.2v4.8.2v4.7.0v4.6.0v4.5.1v4.4.2v4.3.3v4.2.2v4.1.1v4.0.1v3.5.1v3.4.0v3.3.1v3.2.0v3.1.0v3.0.2v2.11.0v2.10.0v2.9.1v2.8.0v2.7.0v2.6.0v2.5.1v2.4.1v2.3.0v2.2.2v2.1.1v2.0.0v1.2.0v1.1.0v1.0.0doc-\\nbuilder-html DEENESFRITKOPTZH Get started 🤗 Transformers Quick tour\\nInstallation Tutorials Run inference with pipelines Write portable code with\\nAutoClass Preprocess data Fine-tune a pretrained model Train with a script Set\\nup distributed training with 🤗 Accelerate Load and train adapters with 🤗 PEFT\\nShare your model Agents Generation with LLMs Task Guides Natural Language\\nProcessing Audio Computer Vision Multimodal Generation Developer guides Use\\nfast tokenizers from 🤗 Tokenizers Run inference with multilingual models Use\\nmodel-specific APIs Share a custom model Run training on Amazon SageMaker\\nExport to ONNX Export to TFLite Export to TorchScript Benchmarks Notebooks\\nwith examples Community resources Custom Tools and Prompts Troubleshoot\\nPerformance and scalability Overview Efficient training techniques Methods and\\ntools for efficient training on a single GPU Multiple GPUs and parallelism\\nEfficient training on CPU Distributed CPU training Training on TPUs Training\\non TPU with TensorFlow Training on Specialized Hardware Custom hardware for\\ntraining Hyperparameter Search using Trainer API Optimizing inference\\nInference on CPU Inference on one GPU Inference on many GPUs Inference on\\nSpecialized Hardware Instantiating a big model Troubleshooting XLA Integration\\nfor TensorFlow Models Optimize inference using `torch.compile()` Contribute\\nHow to contribute to transformers? How to add a model to 🤗 Transformers? How\\nto convert a 🤗 Transformers model to TensorFlow? How to add a pipeline to 🤗\\nTransformers? Testing Checks on a Pull Request Conceptual guides Philosophy\\nGlossary What 🤗 Transformers can do How 🤗 Transformers solve tasks The\\nTransformer model family Summary of the tokenizers Attention mechanisms\\nPadding and truncation BERTology Perplexity of fixed-length models Pipelines\\nfor webserver inference Model training anatomy API Main Classes Agents and\\nTools Auto Classes Callbacks Configuration Data Collator Keras callbacks\\nLogging Models Text Generation ONNX Optimization Model outputs Pipelines\\nProcessors Quantization Tokenizer Trainer DeepSpeed Integration Feature\\nExtractor Image Processor Models Text models Vision models Audio models\\nMultimodal models Reinforcement learning models Time series models Graph\\nmodels Internal Helpers Custom Layers and Utilities Utilities for pipelines\\nUtilities for Tokenizers Utilities for Trainer Utilities for Generation\\nUtilities for Image Processors Utilities for Audio processing General\\nUtilities Utilities for Time Series Join the Hugging Face community and get\\naccess to the augmented documentation experience Collaborate on models,\\ndatasets and Spaces Faster examples with accelerated inference Switch between\\ndocumentation themes Sign Up to get started What 🤗 Transformers can do 🤗\\nTransformers is a library of pretrained state-of-the-art models for natural\\nlanguage processing (NLP), computer vision, and audio and speech processing\\ntasks. Not only does the library contain Transformer models, but it also has\\nnon-Transformer models like modern convolutional networks for computer vision\\ntasks. If you look at some of the most popular consumer products today, like\\nsmartphones, apps, and televisions, odds are that some kind of deep learning\\ntechnology is behind it. Want to remove a background object from a picture\\ntaken by your smartphone? This is an example of a panoptic segmentation task', metadata={'vector': array([-0.01733997, -0.004435  ,  0.00770435, ...,  0.00623866,\n",
       "         -0.01927123, -0.01496727], dtype=float32), 'id': '9a5989a8-4d46-4ec5-afad-bf85b93f1d6e', '_distance': 0.313792884349823}),\n",
       "  Document(page_content='technology is behind it. Want to remove a background object from a picture\\ntaken by your smartphone? This is an example of a panoptic segmentation task\\n(don’t worry if you don’t know what this means yet, we’ll describe it in the\\nfollowing sections!). This page provides an overview of the different speech\\nand audio, computer vision, and NLP tasks that can be solved with the 🤗\\nTransformers library in just three lines of code! Audio Audio and speech\\nprocessing tasks are a little different from the other modalities mainly\\nbecause audio as an input is a continuous signal. Unlike text, a raw audio\\nwaveform can’t be neatly split into discrete chunks the way a sentence can be\\ndivided into words. To get around this, the raw audio signal is typically\\nsampled at regular intervals. If you take more samples within an interval, the\\nsampling rate is higher, and the audio more closely resembles the original\\naudio source. Previous approaches preprocessed the audio to extract useful\\nfeatures from it. It is now more common to start audio and speech processing\\ntasks by directly feeding the raw audio waveform to a feature encoder to\\nextract an audio representation. This simplifies the preprocessing step and\\nallows the model to learn the most essential features. Audio classification\\nAudio classification is a task that labels audio data from a predefined set of\\nclasses. It is a broad category with many specific applications, some of which\\ninclude: acoustic scene classification: label audio with a scene label\\n(“office”, “beach”, “stadium”) acoustic event detection: label audio with a\\nsound event label (“car horn”, “whale calling”, “glass breaking”) tagging:\\nlabel audio containing multiple sounds (birdsongs, speaker identification in a\\nmeeting) music classification: label music with a genre label (“metal”, “hip-\\nhop”, “country”) Copied >>> from transformers import pipeline >>> classifier =\\npipeline(task=\"audio-classification\", model=\"superb/hubert-base-superb-er\")\\n>>> preds =\\nclassifier(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\")\\n>>> preds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"]} for\\npred in preds] >>> preds [{\\'score\\': 0.4532, \\'label\\': \\'hap\\'}, {\\'score\\': 0.3622,\\n\\'label\\': \\'sad\\'}, {\\'score\\': 0.0943, \\'label\\': \\'neu\\'}, {\\'score\\': 0.0903, \\'label\\':\\n\\'ang\\'}] Automatic speech recognition Automatic speech recognition (ASR)\\ntranscribes speech into text. It is one of the most common audio tasks due\\npartly to speech being such a natural form of human communication. Today, ASR\\nsystems are embedded in “smart” technology products like speakers, phones, and\\ncars. We can ask our virtual assistants to play music, set reminders, and tell\\nus the weather. But one of the key challenges Transformer architectures have\\nhelped with is in low-resource languages. By pretraining on large amounts of\\nspeech data, finetuning the model on only one hour of labeled speech data in a\\nlow-resource language can still produce high-quality results compared to\\nprevious ASR systems trained on 100x more labeled data. Copied >>> from\\ntransformers import pipeline >>> transcriber = pipeline(task=\"automatic-\\nspeech-recognition\", model=\"openai/whisper-small\") >>>\\ntranscriber(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\")\\n{\\'text\\': \\' I have a dream that one day this nation will rise up and live out\\nthe true meaning of its creed.\\'} Computer vision One of the first and earliest\\nsuccessful computer vision tasks was recognizing images of zip code numbers\\nusing a convolutional neural network (CNN). An image is composed of pixels,\\nand each pixel has a numerical value. This makes it easy to represent an image\\nas a matrix of pixel values. Each particular combination of pixel values\\ndescribes the colors of an image. Two general ways computer vision tasks can\\nbe solved are: Use convolutions to learn the hierarchical features of an image\\nfrom low-level features to high-level abstract things. Split an image into', metadata={'vector': array([-0.00633756, -0.00267182,  0.02327235, ..., -0.00369172,\n",
       "         -0.02318924, -0.02393728], dtype=float32), 'id': '72570aab-c972-412f-a038-fb6ceb976822', '_distance': 0.3193868398666382}),\n",
       "  Document(page_content='Vision Transformer (ViT) Hugging Face Models Datasets Spaces Docs Solutions\\nPricing Log In Sign Up Transformers documentation Vision Transformer (ViT)\\nTransformers Search documentation\\nmainv4.33.2v4.32.1v4.31.0v4.30.0v4.29.1v4.28.1v4.27.2v4.26.1v4.25.1v4.24.0v4.23.1v4.22.2v4.21.3v4.20.1v4.19.4v4.18.0v4.17.0v4.16.2v4.15.0v4.14.1v4.13.0v4.12.5v4.11.3v4.10.1v4.9.2v4.8.2v4.7.0v4.6.0v4.5.1v4.4.2v4.3.3v4.2.2v4.1.1v4.0.1v3.5.1v3.4.0v3.3.1v3.2.0v3.1.0v3.0.2v2.11.0v2.10.0v2.9.1v2.8.0v2.7.0v2.6.0v2.5.1v2.4.1v2.3.0v2.2.2v2.1.1v2.0.0v1.2.0v1.1.0v1.0.0doc-\\nbuilder-html DEENESFRITKOPTZH Get started 🤗 Transformers Quick tour\\nInstallation Tutorials Run inference with pipelines Write portable code with\\nAutoClass Preprocess data Fine-tune a pretrained model Train with a script Set\\nup distributed training with 🤗 Accelerate Load and train adapters with 🤗 PEFT\\nShare your model Agents Generation with LLMs Task Guides Natural Language\\nProcessing Audio Computer Vision Multimodal Generation Developer guides Use\\nfast tokenizers from 🤗 Tokenizers Run inference with multilingual models Use\\nmodel-specific APIs Share a custom model Run training on Amazon SageMaker\\nExport to ONNX Export to TFLite Export to TorchScript Benchmarks Notebooks\\nwith examples Community resources Custom Tools and Prompts Troubleshoot\\nPerformance and scalability Overview Efficient training techniques Methods and\\ntools for efficient training on a single GPU Multiple GPUs and parallelism\\nEfficient training on CPU Distributed CPU training Training on TPUs Training\\non TPU with TensorFlow Training on Specialized Hardware Custom hardware for\\ntraining Hyperparameter Search using Trainer API Optimizing inference\\nInference on CPU Inference on one GPU Inference on many GPUs Inference on\\nSpecialized Hardware Instantiating a big model Troubleshooting XLA Integration\\nfor TensorFlow Models Optimize inference using `torch.compile()` Contribute\\nHow to contribute to transformers? How to add a model to 🤗 Transformers? How\\nto convert a 🤗 Transformers model to TensorFlow? How to add a pipeline to 🤗\\nTransformers? Testing Checks on a Pull Request Conceptual guides Philosophy\\nGlossary What 🤗 Transformers can do How 🤗 Transformers solve tasks The\\nTransformer model family Summary of the tokenizers Attention mechanisms\\nPadding and truncation BERTology Perplexity of fixed-length models Pipelines\\nfor webserver inference Model training anatomy API Main Classes Agents and\\nTools Auto Classes Callbacks Configuration Data Collator Keras callbacks\\nLogging Models Text Generation ONNX Optimization Model outputs Pipelines\\nProcessors Quantization Tokenizer Trainer DeepSpeed Integration Feature\\nExtractor Image Processor Models Text models Vision models BEiT BiT\\nConditional DETR ConvNeXT ConvNeXTV2 CvT Deformable DETR DeiT DETA DETR DiNAT\\nDINO V2 DiT DPT EfficientFormer EfficientNet FocalNet GLPN ImageGPT LeViT\\nMask2Former MaskFormer MobileNetV1 MobileNetV2 MobileViT MobileViTV2 NAT\\nPoolFormer Pyramid Vision Transformer (PVT) RegNet ResNet SegFormer\\nSwiftFormer Swin Transformer Swin Transformer V2 Swin2SR Table Transformer\\nTimeSformer UperNet VAN VideoMAE Vision Transformer (ViT) ViT Hybrid ViTDet\\nViTMAE ViTMSN ViViT YOLOS Audio models Multimodal models Reinforcement\\nlearning models Time series models Graph models Internal Helpers Custom Layers\\nand Utilities Utilities for pipelines Utilities for Tokenizers Utilities for\\nTrainer Utilities for Generation Utilities for Image Processors Utilities for\\nAudio processing General Utilities Utilities for Time Series Join the Hugging\\nFace community and get access to the augmented documentation experience\\nCollaborate on models, datasets and Spaces Faster examples with accelerated\\ninference Switch between documentation themes Sign Up to get started Vision\\nTransformer (ViT) Overview The Vision Transformer (ViT) model was proposed in\\nAn Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by\\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,', metadata={'vector': array([-0.02194119, -0.01280835,  0.00213298, ..., -0.01619735,\n",
       "         -0.02618968, -0.01556846], dtype=float32), 'id': 'cb20cda3-14d2-419f-bc3e-358251f06e73', '_distance': 0.3212862014770508}),\n",
       "  Document(page_content='Convolutional Vision Transformer (CvT) Hugging Face Models Datasets Spaces\\nDocs Solutions Pricing Log In Sign Up Transformers documentation Convolutional\\nVision Transformer (CvT) Transformers Search documentation\\nmainv4.33.2v4.32.1v4.31.0v4.30.0v4.29.1v4.28.1v4.27.2v4.26.1v4.25.1v4.24.0v4.23.1v4.22.2v4.21.3v4.20.1v4.19.4v4.18.0v4.17.0v4.16.2v4.15.0v4.14.1v4.13.0v4.12.5v4.11.3v4.10.1v4.9.2v4.8.2v4.7.0v4.6.0v4.5.1v4.4.2v4.3.3v4.2.2v4.1.1v4.0.1v3.5.1v3.4.0v3.3.1v3.2.0v3.1.0v3.0.2v2.11.0v2.10.0v2.9.1v2.8.0v2.7.0v2.6.0v2.5.1v2.4.1v2.3.0v2.2.2v2.1.1v2.0.0v1.2.0v1.1.0v1.0.0doc-\\nbuilder-html DEENESFRITKOPTZH Get started 🤗 Transformers Quick tour\\nInstallation Tutorials Run inference with pipelines Write portable code with\\nAutoClass Preprocess data Fine-tune a pretrained model Train with a script Set\\nup distributed training with 🤗 Accelerate Load and train adapters with 🤗 PEFT\\nShare your model Agents Generation with LLMs Task Guides Natural Language\\nProcessing Audio Computer Vision Multimodal Generation Developer guides Use\\nfast tokenizers from 🤗 Tokenizers Run inference with multilingual models Use\\nmodel-specific APIs Share a custom model Run training on Amazon SageMaker\\nExport to ONNX Export to TFLite Export to TorchScript Benchmarks Notebooks\\nwith examples Community resources Custom Tools and Prompts Troubleshoot\\nPerformance and scalability Overview Efficient training techniques Methods and\\ntools for efficient training on a single GPU Multiple GPUs and parallelism\\nEfficient training on CPU Distributed CPU training Training on TPUs Training\\non TPU with TensorFlow Training on Specialized Hardware Custom hardware for\\ntraining Hyperparameter Search using Trainer API Optimizing inference\\nInference on CPU Inference on one GPU Inference on many GPUs Inference on\\nSpecialized Hardware Instantiating a big model Troubleshooting XLA Integration\\nfor TensorFlow Models Optimize inference using `torch.compile()` Contribute\\nHow to contribute to transformers? How to add a model to 🤗 Transformers? How\\nto convert a 🤗 Transformers model to TensorFlow? How to add a pipeline to 🤗\\nTransformers? Testing Checks on a Pull Request Conceptual guides Philosophy\\nGlossary What 🤗 Transformers can do How 🤗 Transformers solve tasks The\\nTransformer model family Summary of the tokenizers Attention mechanisms\\nPadding and truncation BERTology Perplexity of fixed-length models Pipelines\\nfor webserver inference Model training anatomy API Main Classes Agents and\\nTools Auto Classes Callbacks Configuration Data Collator Keras callbacks\\nLogging Models Text Generation ONNX Optimization Model outputs Pipelines\\nProcessors Quantization Tokenizer Trainer DeepSpeed Integration Feature\\nExtractor Image Processor Models Text models Vision models BEiT BiT\\nConditional DETR ConvNeXT ConvNeXTV2 CvT Deformable DETR DeiT DETA DETR DiNAT\\nDINO V2 DiT DPT EfficientFormer EfficientNet FocalNet GLPN ImageGPT LeViT\\nMask2Former MaskFormer MobileNetV1 MobileNetV2 MobileViT MobileViTV2 NAT\\nPoolFormer Pyramid Vision Transformer (PVT) RegNet ResNet SegFormer\\nSwiftFormer Swin Transformer Swin Transformer V2 Swin2SR Table Transformer\\nTimeSformer UperNet VAN VideoMAE Vision Transformer (ViT) ViT Hybrid ViTDet\\nViTMAE ViTMSN ViViT YOLOS Audio models Multimodal models Reinforcement\\nlearning models Time series models Graph models Internal Helpers Custom Layers\\nand Utilities Utilities for pipelines Utilities for Tokenizers Utilities for\\nTrainer Utilities for Generation Utilities for Image Processors Utilities for\\nAudio processing General Utilities Utilities for Time Series Join the Hugging\\nFace community and get access to the augmented documentation experience\\nCollaborate on models, datasets and Spaces Faster examples with accelerated\\ninference Switch between documentation themes Sign Up to get started\\nConvolutional Vision Transformer (CvT) Overview The CvT model was proposed in\\nCvT: Introducing Convolutions to Vision Transformers by Haiping Wu, Bin Xiao,', metadata={'vector': array([-0.01829306, -0.0154734 ,  0.01004938, ...,  0.00272069,\n",
       "         -0.02925223, -0.01033412], dtype=float32), 'id': '7ea7a7ad-0250-43b4-99c8-057b5d39b6b6', '_distance': 0.32782313227653503}),\n",
       "  Document(page_content='choice, and text generation. 🖼️ Computer Vision: image classification, object\\ndetection, and segmentation. 🗣️ Audio: automatic speech recognition and audio\\nclassification. 🐙 Multimodal: table question answering, optical character\\nrecognition, information extraction from scanned documents, video\\nclassification, and visual question answering. 🤗 Transformers support\\nframework interoperability between PyTorch, TensorFlow, and JAX. This provides\\nthe flexibility to use a different framework at each stage of a model’s life;\\ntrain a model in three lines of code in one framework, and load it for\\ninference in another. Models can also be exported to a format like ONNX and\\nTorchScript for deployment in production environments. Join the growing\\ncommunity on the Hub, forum, or Discord today! If you are looking for custom\\nsupport from the Hugging Face team Contents The documentation is organized\\ninto five sections: GET STARTED provides a quick tour of the library and\\ninstallation instructions to get up and running. TUTORIALS are a great place\\nto start if you’re a beginner. This section will help you gain the basic\\nskills you need to start using the library. HOW-TO GUIDES show you how to\\nachieve a specific goal, like finetuning a pretrained model for language\\nmodeling or how to write and share a custom model. CONCEPTUAL GUIDES offers\\nmore discussion and explanation of the underlying concepts and ideas behind\\nmodels, tasks, and the design philosophy of 🤗 Transformers. API describes all\\nclasses and functions: MAIN CLASSES details the most important classes like\\nconfiguration, model, tokenizer, and pipeline. MODELS details the classes and\\nfunctions related to each model implemented in the library. INTERNAL HELPERS\\ndetails utility classes and functions used internally. Supported models ALBERT\\n(from Google Research and the Toyota Technological Institute at Chicago)\\nreleased with the paper ALBERT: A Lite BERT for Self-supervised Learning of\\nLanguage Representations, by Zhenzhong Lan, Mingda Chen, Sebastian Goodman,\\nKevin Gimpel, Piyush Sharma, Radu Soricut. ALIGN (from Google Research)\\nreleased with the paper Scaling Up Visual and Vision-Language Representation\\nLearning With Noisy Text Supervision by Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting\\nChen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yunhsuan Sung, Zhen Li, Tom\\nDuerig. AltCLIP (from BAAI) released with the paper AltCLIP: Altering the\\nLanguage Encoder in CLIP for Extended Language Capabilities by Chen, Zhongzhi\\nand Liu, Guang and Zhang, Bo-Wen and Ye, Fulong and Yang, Qinghong and Wu,\\nLedell. Audio Spectrogram Transformer (from MIT) released with the paper AST:\\nAudio Spectrogram Transformer by Yuan Gong, Yu-An Chung, James Glass.\\nAutoformer (from Tsinghua University) released with the paper Autoformer:\\nDecomposition Transformers with Auto-Correlation for Long-Term Series\\nForecasting by Haixu Wu, Jiehui Xu, Jianmin Wang, Mingsheng Long. Bark (from\\nSuno) released in the repository suno-ai/bark by Suno AI team. BART (from\\nFacebook) released with the paper BART: Denoising Sequence-to-Sequence Pre-\\ntraining for Natural Language Generation, Translation, and Comprehension by\\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman\\nMohamed, Omer Levy, Ves Stoyanov and Luke Zettlemoyer. BARThez (from École\\npolytechnique) released with the paper BARThez: a Skilled Pretrained French\\nSequence-to-Sequence Model by Moussa Kamal Eddine, Antoine J.-P. Tixier,\\nMichalis Vazirgiannis. BARTpho (from VinAI Research) released with the paper\\nBARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese by Nguyen\\nLuong Tran, Duong Minh Le and Dat Quoc Nguyen. BEiT (from Microsoft) released\\nwith the paper BEiT: BERT Pre-Training of Image Transformers by Hangbo Bao, Li\\nDong, Furu Wei. BERT (from Google) released with the paper BERT: Pre-training\\nof Deep Bidirectional Transformers for Language Understanding by Jacob Devlin,\\nMing-Wei Chang, Kenton Lee and Kristina Toutanova. BERT For Sequence', metadata={'vector': array([-0.02489635, -0.02608589, -0.00763404, ...,  0.00137234,\n",
       "         -0.038681  , -0.01463832], dtype=float32), 'id': '57838c39-2fdc-45d3-aa8c-51299609d6fb', '_distance': 0.32936280965805054})]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT, or Parameter-Efficient Fine-Tuning, is a method implemented in the Transformers library. It is designed to efficiently adapt pre-trained language models (PLMs) to various downstream tasks without fine-tuning all of the model's parameters.\n",
      "\n",
      "PEFT methods only fine-tune a small number of additional parameters, significantly reducing computational and storage costs compared to traditional fine-tuning. This is particularly important for large-scale PLMs, which have a large number of parameters that make full fine-tuning prohibitively expensive.\n",
      "\n",
      "PEFT methods, such as LoRA (Low-Rank Adaptation), achieve performance comparable to full fine-tuning while using significantly fewer parameters. This makes it possible to train and deploy models that would otherwise be too large for consumer devices.\n",
      "\n",
      "The Transformers library provides a utility library for PEFT called \"PEFT as a utility library\". It allows users to leverage PEFT's low-level API to inject trainable adapters into any torch module. This API supports adapter types such as LoRA, AdaLoRA, and IA3.\n",
      "\n",
      "By using PEFT, users can fine-tune models more efficiently, reduce computational and storage costs, and achieve comparable performance to full fine-tuning.\n"
     ]
    }
   ],
   "source": [
    "print(answer['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment: Retrieval-Augmented Generation vs Vanilla Generation\n",
    "\n",
    "TODO: Write a python snippet that creates an llm using the ChatOpenAI object with a temperature of 0, and prompt it with `query`. Then, print the output of the llm. `query` is a string that contains the query you want to prompt the llm with and it should ask the language model a question related to the Huggingface library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT (Pretraining with Extracted Feature-based Training) is a method introduced in the transformers library for pretraining language models. It aims to improve the efficiency and effectiveness of pretraining by leveraging extracted features from a pretrained model.\n",
      "\n",
      "In traditional pretraining, models are trained to predict masked tokens in a sentence, which requires a large amount of computation and data. PEFT, on the other hand, reduces the computational cost by using a pretrained model to extract features from a large unlabeled dataset. These features are then used to train a smaller model, known as the \"head\" model, to predict the masked tokens.\n",
      "\n",
      "The PEFT process involves the following steps:\n",
      "\n",
      "1. Pretraining a large model: A large model, such as BERT, is pretrained on a large corpus using the standard masked language modeling objective. This model serves as the \"teacher\" model.\n",
      "\n",
      "2. Feature extraction: The teacher model is used to extract features from a large unlabeled dataset. These features capture the contextual information of the input text.\n",
      "\n",
      "3. Head model training: The extracted features are used to train a smaller \"head\" model. This model is trained to predict the masked tokens using the extracted features as input. The head model is typically smaller and faster than the teacher model.\n",
      "\n",
      "4. Iterative training: The head model is trained iteratively, with the teacher model being used to extract features from the unlabeled data at each iteration. The head model is updated based on the predictions made by the teacher model.\n",
      "\n",
      "PEFT offers several advantages over traditional pretraining methods. It reduces the computational cost by avoiding the need to train a large model from scratch. It also allows for faster experimentation and exploration of different architectures and hyperparameters. Additionally, PEFT can improve the performance of the head model by leveraging the knowledge encoded in the teacher model's extracted features.\n",
      "\n",
      "Overall, PEFT in the transformers library provides a more efficient and effective approach to pretraining language models by leveraging extracted features from a pretrained model.\n"
     ]
    }
   ],
   "source": [
    "# TODO: Write your code here.\n",
    "llm=ChatOpenAI(temperature=0, model='gpt-3.5-turbo-16k')\n",
    "print(llm.predict(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Can you give me a crash course on using the HF Trainer to train a GPT-style model?\"\n",
    "answer = qa({\"query\": query})\n",
    "print(answer['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "query = \"Can you give me a crash course on using the HF Trainer to train a GPT-style model? Provide code snippets for each step.\"\n",
    "answer = qa({\"query\": query})\n",
    "print(answer['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations and Insights\n",
    "Write any observations here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Internet Search Engines to Augment Retrieval\n",
    "\n",
    "Hypothesis: This will be more useful than the previous method for \"how to\" and \"crash course\" requests, because it will allow us to retrieve more relevant documents from the internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q google-api-python-client\n",
    "!pip install -q chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "1. To create an API key: Go to https://console.cloud.google.com/welcome/new and sign in/up using your personal Google account. \n",
    "   - Navigate to the APIs & Services→Credentials panel in Cloud Console. \n",
    "   - Select Create credentials, then select API key from the drop-down menu. \n",
    "   - The API key created dialog box displays your newly created key. \n",
    "   - You now have an API_KEY. Store in it in your `.env` file as `GOOGLE_API_KEY`.\n",
    "\n",
    "2. Setup Custom Search Engine so you can search the entire web \n",
    "   - Create a custom search engine in this [link](http://www.google.com/cse/). \n",
    "   -  In Sites to search, select search the entire web. Select also enable SafeSearch and fill in your search engine name (it can be anything).\n",
    "   - Click on customize.\n",
    "   - Under Search engine ID you’ll find the search-engine-ID. Copy that and add it to your `.env` file as `GOOGLE_CSE_ID`.\n",
    "\n",
    "3. Enable the Custom Search API \n",
    "   - Go to this [URL](https://console.cloud.google.com/apis/library/customsearch.googleapis.com) & click on Enable.\n",
    "   - Alternatively, navigate to the APIs & Services→Dashboard panel in Cloud Console. \n",
    "   - Click Enable APIs and Services. \n",
    "   - Search for Custom Search API and click on it. \n",
    "   - Click Enable. \n",
    "\n",
    "**Note**: Adapted from [GoogleSearchAPIWrapper Docs](https://api.python.langchain.com/en/latest/utilities/langchain.utilities.google_search.GoogleSearchAPIWrapper.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Re-load the environment variables again\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.web_research import WebResearchRetriever\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.utilities import GoogleSearchAPIWrapper\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Vectorstore\n",
    "vectorstore = Chroma(embedding_function=OpenAIEmbeddings(), persist_directory=\"./chromadb_oai\")\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(temperature=0, model='gpt-3.5-turbo-16k', max_tokens=512)\n",
    "\n",
    "search = GoogleSearchAPIWrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "web_research_retriever = WebResearchRetriever.from_llm(\n",
    "    vectorstore=vectorstore,\n",
    "    llm=llm, \n",
    "    search=search,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\erfan\\Documents\\projects\\langchain-crashcourse\\notebooks\\3-retrieval_aug_gen.ipynb Cell 20\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/erfan/Documents/projects/langchain-crashcourse/notebooks/3-retrieval_aug_gen.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mchains\u001b[39;00m \u001b[39mimport\u001b[39;00m RetrievalQAWithSourcesChain\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/erfan/Documents/projects/langchain-crashcourse/notebooks/3-retrieval_aug_gen.ipynb#X25sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m query \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mHow can I finetune a Llama2 7b model on Google Colab?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/erfan/Documents/projects/langchain-crashcourse/notebooks/3-retrieval_aug_gen.ipynb#X25sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m qa_chain \u001b[39m=\u001b[39m RetrievalQAWithSourcesChain\u001b[39m.\u001b[39mfrom_chain_type(llm, retriever\u001b[39m=\u001b[39mweb_research_retriever)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/erfan/Documents/projects/langchain-crashcourse/notebooks/3-retrieval_aug_gen.ipynb#X25sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m result \u001b[39m=\u001b[39m qa_chain({\u001b[39m\"\u001b[39m\u001b[39mquestion\u001b[39m\u001b[39m\"\u001b[39m: query})\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/erfan/Documents/projects/langchain-crashcourse/notebooks/3-retrieval_aug_gen.ipynb#X25sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m result\n",
      "\u001b[1;31mNameError\u001b[0m: name 'llm' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "query = \"How can I finetune a Llama2 7b model on Google Colab?\"\n",
    "qa_chain = RetrievalQAWithSourcesChain.from_chain_type(llm, retriever=web_research_retriever)\n",
    "result = qa_chain({\"question\": query})\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations and Insights\n",
    "Write any observations here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we make it better? We want to be able to retrieve more relevant documents from the internet but also generate complete answers to the user's question without having the user to read the retrieved documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "* https://lancedb.github.io/lancedb/notebooks/code_qa_bot/\n",
    "* https://github.com/langchain-ai/chat-langchain/tree/master"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langcourse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
